[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Intuition for Modern Biologists",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "rnaseq.html",
    "href": "rnaseq.html",
    "title": "1  Introduction to RNA-seq",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to RNA-seq</span>"
    ]
  },
  {
    "objectID": "intro2R.html",
    "href": "intro2R.html",
    "title": "2  Introduction to R",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "3  Introduction to Probability",
    "section": "",
    "text": "4 The Many Flavors of Randomness\nTo understand how statisticians think about randomness, consider an analogy. In biology, everything is complicated and every system we study seems unique. To tackle this complexity, we group things into bins based on important characteristics they share, and then we try to understand the patterns that govern each bin. Bins in biology may be determined by the type of organism being studied (e.g., entymology, mammology, microbiology, etc.), the activities of molecular machines you investigate (e.g., phosphorylating subtrates, regulating transcription, intracellular transport, etc.), and much more. Similarly in statistics, every kind of data seems to possess unique sources and types of variance. Work with enough data though and you will notice that there are patterns in the types of randomness we commonly see. We thus focus on understanding these common patterns and apply them to understanding our unique data.\nThe patterns in randomness that we observe are coined “probability distributions”. Think of them as functions, which take as input a number, and provide as output the probability of observing that number. These functions could take any shape you dream up, but as mentioned, particular patterns crop up all of the time. In the following exercises, we are going to explore these patterns and understand what gives rise to them.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#the-basics",
    "href": "probability.html#the-basics",
    "title": "3  Introduction to Probability",
    "section": "4.1 The basics",
    "text": "4.1 The basics\nTo illustrate some general facts about randomness, we will start by working with two functions in R: rbinom() and rnorm(). These are two of many “random number generators” (RNGs) in R. As we will learn in these exercises, there isn’t just one kind of randomness, so there isn’t just one RNG.\nrbinom and rnorm both have 3 parameters. Both have a parameter called n, which sets the number of random numbers to generate. rbinom has two other parameters called size and prob. size can be any integer &gt;= 0. prob can be any number between 0 and 1 (including 0 and 1). rnorm’s two additional parameters are called mean and sd. mean can be any number, and sd can be any positive number. Take some time to play with both of these functions. As you do so, consider the following:\n\nIn what ways are their output similar?\nIn what ways are their output distinct?\nHow do prob and size impact the output of rbinom?\nHow do mean and sd impact the output of rnorm?\nMake histograms with different numbers of random numbers (i.e., different n). What do you see as n increases? Now repeat this but with different parameters for the relevant function. What changes?\n\n\n4.1.1 Example exercise:\nGenerated 5 draws from each to compare general output:\n\nprint(\"rbinom output:\")\n\n[1] \"rbinom output:\"\n\nrbinom(10, size = 100, prob = 0.5)\n\n [1] 47 42 42 54 42 49 53 39 48 49\n\nprint(\"rnorm output:\")\n\n[1] \"rnorm output:\"\n\nrnorm(10)\n\n [1] -0.43524404 -0.71942904 -2.61952383 -0.40706995 -0.64417850  0.98817250\n [7]  0.53910881 -1.40708906  0.02371572 -0.23719639\n\n\nObservations:\n\nrbinom seems to only output exact integers. rnorm’s output has lots of decimal places\nAs expected, output for both usually differs from run to run\nrnorm output seems to always differ while rbinom might spit out the same number from time to time.\n\nTaking more draws reveals patterns in the randomness that is not as evident from less draws:\nb5 &lt;- rbinom(n = 5, size = 100, prob = 0.5)\nb50 &lt;- rbinom(n = 10, size = 100, prob = 0.5)\nb500 &lt;- rbinom(n = 100, size = 100, prob = 0.5)\nb5000 &lt;- rbinom(n = 1000, size = 100, prob = 0.5)\n\n\nhist(b5, main = \"5 draws\", xlab = \"Random number\")\nhist(b50, main = \"10 draws\", xlab = \"Random number\")\nhist(b500, main = \"100 draws\", xlab = \"Random number\")\nhist(b5000, main = \"1000 draws\", xlab = \"Random number\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVarying size of rbinom:\nb100 &lt;- rbinom(500, size = 100, prob = 0.5)\nb200 &lt;- rbinom(500, size = 200, prob = 0.5)\nb300 &lt;- rbinom(500, size = 300, prob = 0.5)\n\nhist(b100, main = \"size = 100\", xlab = \"Random number\")\nhist(b200, main = \"size = 200\", xlab = \"Random number\")\nhist(b300, main = \"size = 300\", xlab = \"Random number\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher size generates larger random numbers. In fact, distribution of random numbers seems to increase in proportion to increase in size (i.e., doubling size doubles the average random number generated)\nVarying prob of rbinom\nb0.3 &lt;- rbinom(500, size = 100, prob = 0.3)\nb0.5 &lt;- rbinom(500, size = 200, prob = 0.5)\nb0.7 &lt;- rbinom(500, size = 300, prob = 0.7)\n\nhist(b0.3, main = \"prob = 0.3\", xlab = \"Random number\")\nhist(b0.5, main = \"prob = 0.5\", xlab = \"Random number\")\nhist(b0.7, main = \"prob = 0.7\", xlab = \"Random number\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher prob has a similar effect to higher size, increasing the average magnitude of random numbers generated. There also seems to be a similar linear proportionality.\n\n\n4.1.2 Takeaways\n\nThere are two kinds of randomness: discrete and continuous.\nDiscrete randomness is randomness where the output can be described with integers.\nContinuous randomness is randomness where the output can be described with real numbers.\nRandomness is described by its patterns. Any individual data point may be random but collect enough of them from a given process/source and a pattern will emerge. Some values are more likely to occur, some are less likely, and some may even never occur.\n\n\n\n4.1.3 Properties of random variables\n\n4.1.3.1 Mean\nThe mean of a distribution is a measure of its “central tendencies”. Usually, values closer to the mean are more common than values further away from the mean. The mean of a set of random numbers can be calculated in R using the mean() function:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nmean(b)\n\n[1] 50.44\n\nmean(n)\n\n[1] 4.934015\n\n\nBecause of the randomness inherent in RNGs, the mean that you will get for any finite sample will often vary from sample to sample:\n\nb1 &lt;- rbinom(100, size = 100, prob = 0.5)\nb2 &lt;- rbinom(100, size = 100, prob = 0.5)\nb3 &lt;- rbinom(100, size = 100, prob = 0.5)\n\nmean(b1)\n\n[1] 50.13\n\nmean(b2)\n\n[1] 50.33\n\nmean(b3)\n\n[1] 50.5\n\n\nEvery distribution has some inherent mean given its set of parameters. You can think of this as the mean you would get if you took a really large sample. For example, the mean parameter in rnorm is exactly this large sample mean. You can see how the mean of a given sample tends to gets closer to this value as you increase the sample size:\n\nn10 &lt;- rnorm(10, mean = 5)\nn100 &lt;- rnorm(100, mean = 5)\nn1000 &lt;- rnorm(1000, mean = 5)\n\nmean(n10)\n\n[1] 4.821242\n\nmean(n100)\n\n[1] 4.999382\n\nmean(n1000)\n\n[1] 4.949294\n\n\nYou can calculate the mean yourself with some simple R code; it is the sum of all your data points divided by the number of data points:\n\nn100 &lt;- rnorm(100, mean = 5)\n\nmean(n100)\n\n[1] 4.867501\n\nsum(n100)/length(n100)\n\n[1] 4.867501\n\n\nFor any distribution, you can look up its mean (as well as the other properties discussed below) on sites like Wikipedia:\n\nBinomial distribution\nNormal distribution\n\n\n\n4.1.3.2 Variance/standard deviation\nA key feature of randomness is the amount of variability from outcome to outcome of a random process. As we will see when we start using models of data randomness to draw conclusions about said data, figuring out how variable our data is represents a key challenge.\nWe often describe the variability of a random sample using the variance and standard deviation. These can be calculated in R using the var and sd functions, respectively:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nsd(b)\n\n[1] 4.742501\n\nsd(n)\n\n[1] 1.071113\n\nvar(b)\n\n[1] 22.49131\n\nvar(n)\n\n[1] 1.147282\n\n\nClose inspection will reveal that the output of var is just the output of sd squared:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nsd(b)^2\n\n[1] 25.61\n\nvar(b)\n\n[1] 25.61\n\nsd(n)^2\n\n[1] 0.6860808\n\nvar(n)\n\n[1] 0.6860808\n\n\nThe variance measures the squared distance of each data point from the mean, and sums up all of these squared distances:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nvar(b)\n\n[1] 27.74657\n\nsum( ( b - mean(b) )^2 )/(length(b) - 1)\n\n[1] 27.74657\n\nvar(n)\n\n[1] 0.6872376\n\nsum( ( n - mean(n) )^2 )/(length(n) - 1)\n\n[1] 0.6872376\n\n\nWhy the square? When assessing variability, we don’t care if data is above or below the mean, so we want to score data points greater than the mean equally to how we score those below the mean. The square of the mean - 1 is the same as the square of the mean + 1, and both are positive values, because squaring a real number always yields a positive number. Why not the absolute value? Or the fourth power of the distance to the mean? Or any other definitively positive value? Convenience and mathematical formalism. In other words, don’t worry about it.\nNOTE: EXTRA DETAIL ALERT\nThe other mystery of the calculation of the standard deviation/variance is the fact that the sum of the deviations from the mean are divided by the number of data points minus 1, rather than just the number of data points, as in the mean. Why is this? Again, mathematical formalism, but there is an intuitive explanation to be discovered.\nWhen calculating the mean, each data point is its own entity that contributes equally to the calculation. I can’t calculate the mean without all n data points. When calculating the standard deviation though, each data point is compared to the mean, which is itself calculated from all of the data points.\nIf I tell you the mean though, I only need to tell you n - 1 data points for you to infer the final data point. For example, if I have 3 numbers that I tell you average to 2, and two of them are 1 and 3, what is the third? It has to be 2. Thus, when calculating the distance from each point to the mean, there are only n - 1 unique pieces of information. I tell you n - 1 data points and the mean, and you can tell me the standard deviation (since you can infer the missing data point). We say that there are “n - 1 degrees of freedom”. Thus, the effective average of the differences from the mean are all of the squared differences divided by n - 1, the number of “non-redundant” data points that remain after calculating the mean.\nEND OF THE EXTRA INFO ALERT\n\n\n4.1.3.3 Higher order “moments” (BONUS CONTENT)\nThe mean and the variance are related to what are called (who knows why) moments of a distribution. A moment is the average of some function of the random data. For example, the mean is the average of the data itself, which you could call data passed through the function f(x) = x. The variance is related to this moment, as well as the so-called second moment, which is the average of the square of the random data (f(x) = x^2). In fact, this leads to an equivalent way to calculate the variance:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nvar(b)\n\n[1] 23.59788\n\nmean(b^2) - mean(b)^2\n\n[1] 23.3619\n\nvar(n)\n\n[1] 0.9543932\n\nmean(n^2) - mean(n)^2\n\n[1] 0.9448493\n\n\nWell not fully equivalent, but they get closer to one another as the sample size increases:\n\nn100 &lt;- rnorm(100, mean = 5)\nn1000 &lt;- rnorm(1000, mean = 5)\nn10000 &lt;- rnorm(10000, mean = 5)\n\n\n\nn = 100: \nvar(x):  1.031447 \n(x^2) - mean(x)^2:  1.021132 \n \nn = 1000: \nvar(x):  1.009487 \n(x^2) - mean(x)^2:  1.008477 \n \nn = 10000: \nvar(x):  0.993944 \nx^2) - mean(x)^2:  0.9938446 \n \n\n\nVarious other moments or functions of moments enjoy some level of popularity in specific circles. We won’t talk about them here, but you’ll see them on Wikipedia sites for distributions, things like the “skewness” and “excess kurtosis” are examples of these additional moments, or functions of moments.\n\n\n4.1.3.4 Density/mass functions\nEarlier, I said “Think of [probability distributions] as functions, which take as input a number, and provide as output the probability of observing that number”. Fleshing out what I mean by this will help you understand one of the key concepts in probability theory: probability density/mass functions.\nTake a look at what a normalized histogram of draws from rbinom() looks like as we increase n. “Normalized” here means that instead of the y-axis representing the number of draws in a given bin (what is plotted by hist() by default), it is this divided by the total number of draws.\nb5 &lt;- rbinom(n = 10, size = 100, prob = 0.5)\nb50 &lt;- rbinom(n = 100, size = 100, prob = 0.5)\nb500 &lt;- rbinom(n = 1000, size = 100, prob = 0.5)\nb5000 &lt;- rbinom(n = 10000, size = 100, prob = 0.5)\n\n\n# freq = FALSE will plot normalized counts\nhist(b5, main = \"10 draws\", xlab = \"Random number\", freq = FALSE)\nhist(b50, main = \"100 draws\", xlab = \"Random number\", freq = FALSE)\nhist(b500, main = \"1000 draws\", xlab = \"Random number\", freq = FALSE)\nhist(b5000, main = \"10000 draws\", xlab = \"Random number\", freq = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you increase n, a pattern begins to emerge. The y-axis is the fraction of the time a random number ended up in a particular bin. It can thus be interpreted as the probability that the random number falls within the bounds defined by that bin. What you may notice is that these probabilities seem to converge to particular values as n gets large. This pattern of probabilities is known as the binomial distributions “probability mass function”. If the output of the RNG is continuous real numbers, then this is referred to as the “probability density function”.\nFor all of the distributions that we can draw random numbers from in R, we can similarly use R to figure out its probability mass/density function. The function will have the form d&lt;distribution&gt;, where d denotes density function:\n\ndbinom(x = 50, size = 100, prob = 0.5)\n\n[1] 0.07958924\n\ndbinom(x = 10, size = 100, prob = 0.5)\n\n[1] 1.365543e-17\n\ndbinom(x = 90, size = 100, prob = 0.5)\n\n[1] 1.365543e-17\n\ndbinom(x = 110, size = 100, prob = 0.5)\n\n[1] 0\n\ndbinom(x = 50.5, size = 100, prob = 0.5)\n\nWarning in dbinom(x = 50.5, size = 100, prob = 0.5): non-integer x = 50.500000\n\n\n[1] 0\n\n\n\n4.1.3.4.1 Exercise\nProve to yourself that dbinom yield probabilities similar to what you saw in your normalized histograms.\nHint:\n\n### Make bins of size 1 so as to match up more cleanly with `dbinom()`\n\n# Maximum value\nsize &lt;- 100\n\n# Plot with bin size of 1\nhist(rbinom(1000, size, 0.5), freq = FALSE, breaks = 1:size)\n\n\n\n\n\n\n\n\nEXAMPLE EXERCISE:\nlibrary(ggplot2)\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Histogram\nrns &lt;- rbinom(10000, 100, 0.5)\n\n# dbinom\nvalues &lt;- 0:100\nprobs &lt;- dbinom(values, size = 100, prob = 0.5)\n\ntibble(x = rns) %&gt;%\n  ggplot(aes(x = x)) +\n  theme_classic() + \n  geom_histogram(binwidth = 1, aes(y=..count../sum(..count..))) +\n  xlab(\"Value\") +\n  ylab(\"Proportion\") +\n  xlim(c(0, 100)) + \n  ylim(c(0, max(probs + 0.01)))\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\ntibble(x = values,\n       y = probs) %&gt;%\n  ggplot(aes(x = x, y = y)) + \n  geom_bar(stat = \"identity\") + \n  theme_classic() +\n  xlab(\"Value\") +\n  ylab(\"Probability\") +\n  xlim(c(0, 100)) + \n  ylim(c(0, max(probs + 0.01)))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\nEND OF EXAMPLE EXERCISE\nIn the case of discrete random variables, this exercise shows that the output of d&lt;distribution&gt;() has a clear interpretation: its the probability of seeing the specified value given the specified distribution parameters. In the case of continuous random variables though, the interpretation is a bit trickier. For example, consider an example output of dnorm():\n\ndnorm(0, mean = 0, sd = 0.25)\n\n[1] 1.595769\n\n\nIt’s greater than 1! How could a probability be greater than 1?? The answer is that the output isn’t a probability in the same way as in the discrete case. All that matters for this class is that this number is proportional to a probability. The distinction here is a bit unintuitive, but there are some great resources for fleshing out what this means. See for example 3blue1brown’s video on the topic.\nBEGINNING OF BONUS CONTENT\nHere I will briefly explain what the output of dnorm() represents.\nThe output of dnorm() represents a “probability density”, hence the name “probability density function”. What is a “probability density”? The analogy to an object’s mass and density is fitting. If you want to get something’s mass given its density, you need to multiply its density by that object’s volume. To get probability (mass) from a density (output of dnorm()) you need to specify how large of a bin to consider.\n\ndnorm(0, mean = 0, sd = 0.25)*0.001\n\n[1] 0.001595769\n\n\nThis can be interpreted as the probability of a number generated by rnorm() falling between -0.0005 (i.e., 0 - 0.001/2) and 0.0005. This is only an approximation, as the density (output of dnorm()) is not constant between -0.0005 and 0.0005:\n\ndnorm(-0.0005, mean = 0, sd = 0.25)\n\n[1] 1.595766\n\ndnorm(0, mean = 0, sd = 0.25)\n\n[1] 1.595769\n\n\nChoose a small enough bin though, and it will be pretty close to the probability of a number ending up in that bin. The exact answer to this question comes from integrating the probability density function within the bin of interest:\n\\[\n\\text{P(x} \\in [L, U]\\text{)} = \\int_{L}^{U} \\text{dnorm(x, mean = 0, sd = 0.25) dx}  \n\\]\n\\(\\text{P(x} \\in [L, U]\\text{)}\\) should be read as “the probability that a random variable x drawn from rnorm() falls between \\(L\\) and \\(U\\)”.\n!!END OF LECTURE 1!!",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#discrete-random-variables",
    "href": "probability.html#discrete-random-variables",
    "title": "3  Introduction to Probability",
    "section": "4.2 Discrete random variables",
    "text": "4.2 Discrete random variables\n\n4.2.1 Modeling two outcomes: the binomial distribution\nYou have already played around with the binomial distribution, as that is the name given to the distribution of numbers generated by rbinom. Now it is time to tell its story. All distributions have a story, or rather, a generative model. A generative model is a process that would give rise to data following a particular distribution.\nThe binomial’s story goes like this: imagine you have an event that can result in two possible outcomes. For example, flipping a coin (an event) can result in either a heads or a tails (two possible outcomes). One thing has to be true about this process for it to be well described by a binomial distribution: the probability of a particular outcome must be exactly the same from event to event. For example, if every time you flip a coin, it has a 50% chance of coming up heads and a 50% chance of coming up tails, then the number of heads is well described by a binomial distribution.\nThe size parameter in the rbinom function sets the number of events you want to simulate. The prob parameter sets the probability of an outcome termed the “success”. Success here has a misleading connotation; it might represent an outcome you are happy about, it might represent an outcome that you displeased by, it might represent an outcome that you are completely indifferent to. Statisticians name things in funny ways…\n\n4.2.1.1 Exercise\nTake some time to explore the properties and output of rbinom(). Questions to consider include:\n\nHow does the mean depend on size?\nHow does the mean depend on prob?\nHow does the variance depend on size? prob?\nWhat is the most likely outcome for a given size and prob?\nWhat is an aspect of RNA-seq data that you could model with a binomial distribution?\n\n\n\n4.2.1.2 Example exercise:\nGoing hardcore: inspect properties of distribution as function of size and prob\nlibrary(dplyr)\nlibrary(ggplot2)\n\nnsims &lt;- 5000\n\nprobs &lt;- seq(from = 0, to = 1, by = 0.1)\nsizes &lt;-  seq(from = 0, to = 100, by = 10)\n\nLp &lt;- length(probs)\nLs &lt;- length(sizes)\n\n\nmeans &lt;- rep(0, times = Lp*Ls)\nvars &lt;- means\n\ncount &lt;- 1\nfor(p in seq_along(probs)){\n  \n  for(s in seq_along(sizes)){\n    \n    simdata &lt;- rbinom(nsims, size = sizes[s], prob = probs[p])\n    \n    means[count] &lt;- mean(simdata)\n    vars[count] &lt;- var(simdata)\n    count &lt;- count + 1\n    \n  }\n  \n}\n\n\nsim_df &lt;- dplyr::tibble(prob = rep(probs, each = Ls),\n                        size = rep(sizes, times = Lp),\n                        mean = means,\n                        variance = vars) \n\nsim_df %&gt;%\n  ggplot(aes(x = size, y = prob, fill = mean)) + \n  geom_tile() + \n  scale_fill_viridis_c() + \n  theme_classic() + \n  xlab(\"Size\") + \n  ylab(\"Prob\")\nsim_df %&gt;%\n  ggplot(aes(x = size, y = prob, fill = variance)) + \n  geom_tile() + \n  scale_fill_viridis_c() + \n  theme_classic() + \n  xlab(\"Size\") + \n  ylab(\"Prob\")\n\n\n\n\n\n\n\n\n\n\nObservations:\n\nConfirms that higher size and prob = higher average number\nInteresting to see that variance increases as a function of size, but that prob = 0.5 leads to the highest variance.\nSetting prob to 0 or 1 causes variance to go exactly 0, regardless of what size is set to. Similarly, setting prob to 0 causes the mean to go to exactly 0, regardless of what size is set to.\n\n\n\n\n4.2.2 Modeling &gt; 2 outcomes: the multinomial distribution\nWhen exploring the binomial distribution, we considered data with two possible outcomes. What if many of the same assumptions hold, but there are now more than 2 possible results? That is where the multinomial distribution comes in.\nGenerative model: Imagine you have an event that can result in one of N outcomes, where N is some integer. Each outcome has some probability, p_{i} (i denoting the ith outcome, for i between 1 and N, inclusive) of occurring, and all of the p_{i} remain constant from event to event.\nExample case: When rolling a die, there are 6 outcomes of what number shows face up when the die stops rolling (1, 2, 3, 4, 5, or 6). If each face is equally likely to show up with each roll, then p_{i} = 1/6 for all i. This can be simulated in R with rmultinom():\n\nrmultinom(5, size = 1, prob = rep(1/6, times = 6))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    0    0    1    0    0\n[3,]    0    1    0    1    0\n[4,]    1    0    0    0    0\n[5,]    0    0    0    0    1\n[6,]    0    0    0    0    0\n\n\nThe output will be a matrix with n columns and number of rows equal to the length of prob. The value in the ith row and jth column is the number of times event i was observed in simulation j. In this case, the rows can be interpreted as the number a die came up, and each columns represents the result of a single role of a single die.\n\n4.2.2.1 Exercise\nUse rmultinom() and a bit of ancillary code to simulate the sequence of an RNA produced from a gene. This doesn’t need to be a real gene, just a random sequence of nucleotides. Then determine some features of this sequence:\n\nWhat is the longest run of each nucleotide in a row?\nWhat is the least common nucleotide in your sequence?\nWhat is the most common nucleotide in your sequence?\nHow different is the rate of occurence of the least and most common nucleotides\n\n\n\n4.2.2.2 Example exercise\n\n### Parameters for simulation\n\n# Number of nucleotides in RNA\nlength &lt;- 25\n\n# Nucleotide proportions\nATprop &lt;- 0.4\nCGprop &lt;- 0.6\n\n### Generate random nucleotides\nnucleotides &lt;- rmultinom(length, size = 1,\n                         prob = c(ATprop/2, ATprop/2,\n                                  CGprop/2, CGprop/2))\n\n### Determine sequence\nnucleotide_types &lt;- c(\"A\", \"U\", \"C\", \"G\")\nnucleotide_ids &lt;- 1:4\n\n# Cute matrix algebra trick\n  # IDs will be row ID for which value of matrix was 1\nIDs &lt;- t(nucleotides) %*% matrix(1:4, nrow = 4, ncol = 1)\nsequence &lt;- paste(nucleotide_types[IDs], collapse = \"\")\n\n\nprint(sequence)\n\n[1] \"CUCCUUGACAGUGAACGUCGGCGUG\"\n\n\n\n\n\n4.2.3 Modeling “counts”: the Poisson distribution\nAs we have seen, RNA-seq data can be summarized as a matrix of read counts. Read counts are the number of times that we observed an RNA fragment from a particular genomic feature. What then, is a good model for the number of read counts for a particular feature? Enter a major contender: the Poisson distribution\nGenerative model: Imagine that “events” are independent, that is the time since the last event or how many such events have already occurred have no bearing on how likely we are to see another event. The number of such events in a particular period of “time” (time here in quotes as time could represent units like “number of RNA fragments sequenced”) will be Poisson distributed.\n\n4.2.3.1 Exercise\n\nExplore the output of rpois(), the Poisson distribution random number generator. Besides the typical n parameter, it has only one other parameter, lambda. Investigate the impact of changing lambda.\nCompare the amount of variability in rpois() output to the amount of variability in a real RNA-seq dataset.\n\n\n\n\n4.2.4 Modeling time to first “success”: the geometric distribution\nThe binomial distribution provides a great model for the number of “successes” in a set of random, binary outcomes. What if we were interested in how long we will have to wait until the first success though? That is where the geometric distribution comes into play.\nGenerative Model: Like with the binomial distribution, imagine you have an event that can result in two possible outcomes, and the probability of a particular outcome is exactly the same from event to event. The number of events until you see until the first success will follow a geometric distribution.\n\n4.2.4.1 Exercise\n\nExplore the output of rgeom(), the geometric distribution random number generator. Besides the typical n, it has only one additional parameter, prob, which can be a number between 0 and 1.\nUse rbinom() and a bit of custom R code to create your own rgeom() alternative, and confirm that the average value given by your simulator and rgeom() for a given prob is approximately equal.\n\n\n\n\n4.2.5 Modeling time to nth “success”: the negative binomial distribution\nThe geometric distribution describes the time until the first success of a binary outcome. What if we need to model the time to the nth success, where n is any integer &gt; 1? For that, we can look to the negative binomial distribution.\nGenerative Model: Like with the binomial distribution, imagine you have an event that can result in two possible outcomes, and the probability of a particular outcome is exactly the same from event to event. The number of events until you see until the nth success will follow a negative binomial distribution.\nSpoiler alert: We’ll see the negative binomial later, but it will take on a very different character, acting as a generalization of the Poisson distribution rather than of the geometric distribution. This just goes to show that these distributions can wear many hats and will often have many distinct back stories.\n\n4.2.5.1 Exercises\n\nExplore the output of rnbinom(). In addition to the standard n, you should play around with the two rnbinom() unique parameters relevant to the generative model laid out above: size (number of successes) and prob (probability of a success).\nCreate your own rnbinom() simulator using rbinom() and some custom R code. Compare the properties of your simulator and rnbinom() to ensure things are working as expected.\n\n\n\n\n4.2.6 Sampling without replacement: the hypergeometric distribution\nThe last major discrete distribution to discuss is the hypergeometric distribution. Every single distribution we have looked at thus far has an assumption of event-to-event independence. That is, no matter what has happened previously, the probability of future events is unwaivering. Can you imagine any case where this might not be true?\nThe simplest way to violate this assumption is to consider the process of “sampling without replacement”. Say you have a bag of marbles of two colors, black and red. Now imagine that you draw one from the bag, assess its color. What distribution describes the number of black marbles you see? If each time you draw a marble, you subsequently return it to the bag, then the number of black marbles would be well modeled as binomially distributed. This would be called “sampling with replacement”, and it yields independence between each draw. The contents of the bag never changes so neither do the probabilities of a particular result. What if you DIDN’T return each marble though? This would be called “sampling without replacement”, and now the result of the last draw matters, because it affects how likely each outcome is in the next draw. Describing the number of black marbles in this case requires a new distribution: the hypergeometric distribution.\n\n4.2.6.1 Exercise\n\nExplore the output of rhyper. Its parameters are nn (what is referred to as n in all of the obther r&lt;dist&gt;() functions), m (think of this as the number of red marbles), n (think of this as the number of black marbles), and k (think of this as the number of marbles drawn from the bag).\n\n!!END OF LECTURE 2!!",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#continuous-random-variables",
    "href": "probability.html#continuous-random-variables",
    "title": "3  Introduction to Probability",
    "section": "4.3 Continuous random variables",
    "text": "4.3 Continuous random variables\nUp until now, we have focused on describing “discrete randomness”. This means that outcomes of the processes that we considered had to be well described with integers (0, 1, 2, …). Not everything in the world fits this description though. Consider the following types of data/processes we could imagine modeling:\n\nThe heights of college-aged students\nThe probability of contracting COVID\n\n\nIn these cases, the outcome is best described using a real number, that is a number which can have an arbitrary number of decimal places. We refer to these as “continuous random variables”, and in this lecture we will familiarize ourselves with the most popular distributions for modeling such processes.\n\n4.3.1 Pure randomness: the uniform distribution\nWhat’s the first thing that comes to mind when you hear that something is “random”? At this point, you should be conditioned to start thinking about the underlying generative model and the distribution that could describe said randomness, but what if you had been asked about randomness embarking on this class? I would argue that for most people, the default definition of randomness is “pure randomness”: every possible event is equally likely. While this class should ensure this is no longer your default, there are plenty of processes that are well described by this sort of randomness. For those cases, we have the uniform distribution.\nGenerative model: Imagine the output of a process can be described as a real number between a and b. If all values between a and b are equally likely, then this process’ output will be well modeled with a uniform distribution\n\n4.3.1.1 Exercise\n\nExplore the output of runif(). It has 3 parameters of note. The first is the same for all RNGs and is called n. It represents the number of random numbers it spits out. The other two are called min and max. Both of these can be any number you want, as long as min &lt;= max. Generate some random numbers with runif and observe there properties.\n\n\n\n\n4.3.2 Generalizing the uniform distribution\n\n4.3.2.1 Exercises\n\nExplore the output of rbeta(). Check out ?rbeta() to see what parameters exist.\nCombine rbeta() and rbinom(), using the former to simulate values of p for the latter. Compare this to rbinom() with prob = shape1 / (shape1 + shape2). How are they similar? How do they differ?\n\n\n\n\n4.3.3 Modeling the time until an event: the exponential distribution\n\n4.3.3.1 Exercises\n\nExplore the output of rexp(). Check out ?rexp() to see what parameters exist.\nCompare the following distributions:\n\n\nThe full output of a run of rexp() with a large n, like 100,000\nThat same output, but subtracting the first quartile from all of the values, and throwing out any negative values. You can find what the first quartile is with quartile().\n\n\nDo the same thing as in 2 but with the output of rbeta(). Do you get the same result as in 2?\nDo the same thing as in 2 and 3 but with the output of rgeom(). Do you get the same result as in 2 or as in 3?\n\nExercises 2-4 explore the property known as memorylessness, which is only possessed by two distributions in the entire universe of distributions. This property in some sense defines these two distributions.\n\n\n\n4.3.4 Generalizing the exponential distribution: the gamma distribution\n\n4.3.4.1 Exercises\n\nCompare the output of rgamma() with shape = 1 to that of rexp(). Make some plots to convince yourself that these are the same.\nExplore how changing the shape parameter affects how the output of rgamma() differs from that of rexp().\n\n\n\n\n4.3.5 All distributions lead here: the normal distribution\n\n4.3.5.1 Exercises\n\nChoose your favorite continuous distribution to this point. Use its associated RNG to follow these steps:\n\n\n\nGenerate N samples from your distribution of choice with whatever parameters you desire.\n\n\nCalculate the average of those N samples with mean()\n\n\nRepeat a) and b) M times and save the result of b) each time. You should use a for loop for this.\n\n\nPlot a histogram of the sample means.\n\n\nCompare your plot to a histogram from rnorm(M, mean = E, sd = S), where E = mean(means) and S = sd(means).\n\n\nExplore the impact of varying N, M, and the parameters of your distribution of choice on the plots generated in e).\n\n\n!!END OF LECTURE 3!!",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#problem-set-simulating-data-with-distributions",
    "href": "probability.html#problem-set-simulating-data-with-distributions",
    "title": "3  Introduction to Probability",
    "section": "4.4 Problem set: Simulating data with distributions",
    "text": "4.4 Problem set: Simulating data with distributions\n\n4.4.1 RNA-seq data\nTry and simulate a count matrix that has similar properties to that of a real RNA-seq count matrix. This is a fairly open-ended exercise and is meant as an exercise in thinking carefully about modeling data with probability distributions. Compare your simulated data to the real thing, making some plots to assess their similarity.\n\n\n4.4.2 Poisson Process\nThis exercise will teach you how to use what you have learned to simulate what is known as a Poisson process. The strategy employed here is known as Gillespie’s algorithm, and is widely used in the computational modeling of biochemical reactions.\nImplement a simulation of RNA transcription following these steps:\n\nSpecify two parameters at the top of your code:\n\n\nT: the length of time for which to simulate.\nrate: The rate at which RNA molecules are synthesized.\n\n\nSet a couple variables that will change throughout the simulation:\n\n\ncurrent_t: the current time in the simulation. Set to 0.\nRNA_cnt: the number of RNAs that have been produced. Set to 0.\n\n\nIn a while loop, add the output of rexp(n = 1, rate = rate) to current_t. If the new value of current_t is &gt; T, break out of the while loop. If not, then add one to RNA_cnt.\n\nExplore some properties of your simulation:\n\nMake a plot of the number of RNAs as a function of time. Make this plot with three different values of rate.\nRun the simulation multiple times with a given T and rate, and compare the distribution of RNA_cnt’s to that of a Poisson distribution with lambda = rate. Are they the same? You are discovering why this is called a Poisson process.\n\nEXTRA CREDIT\nSimulate RNA synthesis and degradation, making a plot of the amount of undegraded RNA as a function of time. Some things you will need to know:\n\nIf you have multiple processes whose time until the next event is exponentially distributed, the time until any of these events occurs is also exponentially distributed, with rate = sum of the rates of all of the individual exponential distributions.\nIf you have multiple processes whose time until the next event is exponentially distributed, the probability that event i is the next event = \\(rate_{i}/\\sum rate_{j}\\), where \\(rate_{i}\\) represents the rate parameter for the ith events exponential distribution, and the sum is over the rates of all of the processes’ associated exponential distributions.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#appendix-probability-distributions",
    "href": "probability.html#appendix-probability-distributions",
    "title": "3  Introduction to Probability",
    "section": "Appendix: Probability Distributions",
    "text": "Appendix: Probability Distributions\nWhen I claim that “there is randomness in your data”, what does that mean? For some people, the term “randomness” implies complete unpredictability. Such people would interpret my claim to mean that every time you collect a new replicate, any value for the thing you are measuring is fair game and equally likely. “You got 100 reads from the MYC gene in your last RNA-seq dataset? Well don’t be surprised if you get 1000, or 10000, or 0 reads next time!” You may call this uniform randomness. You can generate such data right here in R, using the runif() function:\n\ndata &lt;- runif(n = 10, min = 0, max = 100)\n\nhist(data)\n\n\n\n\n\n\n\n# Check out the individual data points\nprint(data)\n\n [1] 70.97239 98.38451 71.91218 36.48995 58.96500 84.78097 15.68629 33.77933\n [9] 98.73831 45.35082\n\n\nrunif() has three parameters: 1) n specifies the number of numbers to generate, 2) min specifies the minimum number it could possibly generate, and 3) max specifies the maximum number it could possibly generate. In the above example, I am thus creating 10 numbers between 0 and 100, with every number in between being equally likely to pop out. Generate a lot more numbers and this uniform pattern of appearance becomes much more clear:\n\ndata &lt;- runif(n = 1000, min = 0, max = 100)\n\nhist(data)\n\n\n\n\n\n\n\n\nWhile this definition of randomness is intuitive, it can’t be the only type of randomness. If RNA-seq data were this random, it would be useless! There is nothing to learn from measurements that can take on any value with equal probability.\nThus, to describe all of the kinds of randomness we see in the real world, it is important to expand our definition beyond uniform randomness. Enter the probability distribution. A probability distribution is like a function in R. It takes as input a number (or maybe a set of numbers), and provides as output, the probability of seeing that number. For uniformly random data this might look something like:\n\nuniform_distribution &lt;- function(data, min = 0, max = 100){\n  \n  if(data &gt;= min & data &lt;= max){\n    \n    output &lt;- 1\n    \n  }else{\n    \n    output &lt;- 0\n    \n  }\n  \n  return(output)\n  \n}\n\nThat is to say, as long as the data is within the bounds of what is possible, it has the same probability of occurring; you get the same number out from this function. This function would make mathematicians",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "4  Introduction to Statistical Modeling",
    "section": "",
    "text": "4.1 Developing an intuition for model fitting\nThis exercise will walk you through how to think about what it means to “fit” a model. The key tool in our arsenal is going to be simulation, where we create data that follows known distributions. Your task, if you are willing to accept it, is to figure out how to recover the parameters you used in your simulation from the noisy data generated by the simulator. Are you ready? I’ll start with a guided exercise, and then present some open-ended exercises to get you exploring model fitting.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Statistical Modeling</span>"
    ]
  },
  {
    "objectID": "modeling.html#developing-an-intuition-for-model-fitting",
    "href": "modeling.html#developing-an-intuition-for-model-fitting",
    "title": "4  Introduction to Statistical Modeling",
    "section": "",
    "text": "4.1.1 Guided exercise\nStep 0: Devising a model\nLet’s say you would like to estimate what fraction of the nucleotides in the actual RNA population are purines (Gs and As). Each nucleotide in an RNA can either be a purine or a pyrimidine. How would you go about answering this question?\nStep 1: Simulate some data\nFor this exercise, I am going to simulate binomially distributed data, and devise a strategy to figure out what prob I used in this simulation. Obviously, I will know what prob I used, but in the real world, you won’t have access to the true parameters of the universe. Knowing the truth helps us know if we are on the right track though, and is why simulations are such a useful tool:\n\n# Set the seed so you can reproduce my results\nset.seed(42)\nsimdata &lt;- rbinom(100, 100, 0.5)\n\nTechnically, there are two parameters that I had to set here, size and prob. In this case, I am going to assume that size is data I have access to. Usually, if we are modeling something as binomially distributed, we will know how many trials there were.\nStep 2: Visualize your data\nLet’s see what the data looks like:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsim_df &lt;- tibble(successes = simdata)\n\nsim_plot &lt;- sim_df %&gt;%\n  ggplot(aes(x = successes)) + \n  geom_histogram() +\n  theme_classic()\nsim_plot\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nDoes the data look at all surprising? We set prob equal to 0.5, so on average, we expect 50% of the trials to end in successes. If we perfectly hit this mark in our finite sample, that would mean 50 successes. Let’s annotate this mark on the plot\n\nsim_plot +\n  geom_vline(xintercept = 50,\n             color = 'darkred')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIt’s a bit noisy, but the data does seem to be roughly centered around 50. That’s a good sign that our simulation worked, but now we need to think about how we would have figured out that the prob in this case was 0.5\nStep 3: Fight simulations with simulations\nOur goal is to find a binomial distribution prob parameter that accurately describes this data. An intuitive way to think about doing this is to simulate data with candidate values for prob, and see how similar the simulated data is to the real data. Here’s how you might do that:\n\n### With a for loop\nlibrary(tidyr)\ncandidates &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9)\n\nsim_list &lt;- vector(mode = \"list\",\n                  length = length(candidates))\nfor(c in seq_along(candidates)){\n  \n  sim_list[[c]] &lt;- rbinom(100, 100, candidates[c])\n  \n}\nnames(sim_list) &lt;- as.factor(candidates)\nsim_df &lt;- as_tibble(sim_list) %&gt;%\n  pivot_longer(names_to = \"prob\",\n               values_to = \"successes\",\n               cols = everything())\n\nsim_df %&gt;%\n  ggplot(aes(x = successes, \n             fill = prob, \n             color = prob)) + \n  geom_histogram(alpha = 0.1,\n                 position = 'identity') +\n  geom_histogram(data = tibble(successes = simdata,\n                               prob = factor('data')),\n                 aes(x = successes),\n                 color = 'black',\n                 alpha = 0.1,\n                 fill = 'darkgray') +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() + \n  theme_classic() +\n  ggtitle('data in black/gray') + \n  xlim(c(0, 100))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 10 rows containing missing values (`geom_bar()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\nsim_df %&gt;%\n  ggplot(aes(x = successes, \n             fill = prob, \n             color = prob)) + \n  geom_density(alpha = 0.1,\n                 position = 'identity') +\n  geom_density(data = tibble(successes = simdata,\n                               prob = factor('data')),\n                 aes(x = successes),\n                 color = 'black',\n                 alpha = 0.1,\n                 fill = 'darkgray') +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() + \n  theme_classic() +\n  ggtitle('data in black/gray') + \n  xlim(c(0, 100))\n\n\n\n\n\n\n\n\nOf these small subset of candidate prob values, 0.5 is the clear winner. The overlap of the simulation with the data is striking, and strongly suggests that this is a good fit to our data.\nOf course, we know the true value is 0.5, and this knowledge guided our choice of candidates. So let’s explore a different range of candidates, all of which are much closer to the known truth:\n\n### With a for loop\nlibrary(tidyr)\ncandidates &lt;- c(0.48, 0.49, 0.5, 0.51, 0.52)\n\nsim_list &lt;- vector(mode = \"list\",\n                  length = length(candidates))\nfor(c in seq_along(candidates)){\n  \n  sim_list[[c]] &lt;- rbinom(100, 100, candidates[c])\n  \n}\nnames(sim_list) &lt;- as.factor(candidates)\nsim_df &lt;- as_tibble(sim_list) %&gt;%\n  pivot_longer(names_to = \"prob\",\n               values_to = \"successes\",\n               cols = everything())\n\n\nsim_df %&gt;%\n  ggplot(aes(x = successes, \n             fill = prob, \n             color = prob)) + \n  geom_histogram(alpha = 0.1,\n                 position = 'identity') +\n  geom_histogram(data = tibble(successes = simdata,\n                               prob = factor('data')),\n                 aes(x = successes),\n                 color = 'black',\n                 alpha = 0.1,\n                 fill = 'darkgray') +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() + \n  theme_classic() +\n  ggtitle('data in black/gray') + \n  xlim(c(0, 100))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 10 rows containing missing values (`geom_bar()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\nsim_df %&gt;%\n  ggplot(aes(x = successes, \n             fill = prob, \n             color = prob)) + \n  geom_density(alpha = 0.1,\n                 position = 'identity') +\n  geom_density(data = tibble(successes = simdata,\n                               prob = factor('data')),\n                 aes(x = successes),\n                 color = 'black',\n                 alpha = 0.1,\n                 fill = 'darkgray') +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() + \n  theme_classic() +\n  ggtitle('data in black/gray') + \n  xlim(c(0, 100))\n\n\n\n\n\n\n\n\nNow the comparisons are much messier. Sure, 0.5 is a great match, but none of these chosen values yield simulations too different from the truth.\nFrom this, it is clear that there are ranges of values that we can confidently rule out as good conclusions for what prob best describes our data. Values of 0.25 or less, and values of 0.75 or more are very bad fits. There is also good evidence that a good fit is somewhere around 0.5, but the exact value that would represent the best guess is uncertain.\nStep 4: Make fitting more rigorous\nThe current strategy we have employed could be referred to as “simulations and vibes”. We have simulated data with some values for the unknown parameter in question, and assessed by eye how close our simulated data matched our real data. Whlie this has offered some valuable insights, it’s important to recognize the fundamental limitations of such an approach:\n\nEach simulation with a given parameter value will yield different results. Random number generators are going to be random. One output of prob = 0.5 might look exactly like the real think, but so might one value of prob = 0.48.\nIt’s time intensive. You have to simulate data for a bunch of different values, and then painstakingly stare at cluttered plots trying to make sense of which simulation fit your data the best.\nIt’s subjective. None of our simulations matched the data exactly, and even if one run of a particular simulation did, see point 1 for why that can’t be considered conclusive evidence in favor of that parameter being the best. Lacking such a perfect match, we are forced to rely on ill-defined, self-imposed criteria for what the best fit is.\n\nHow can we fix this problem? What we need is a quantitative metric, a number that we can assign to every possible value of prob that describes how good of a fit it is to our data. It should have the following properties:\n\nIt should be deterministic; a given value of prob should yield a single unique value for this metric.\nHigher values should represent better fits.\nThe more data we have, the better this metric should do at predicting the true parameter value.\n\nSit and think about this for a while (maybe a couple decades, which is what it took the field to converge on this solution), and you’ll arrive at something called the “total likelihood”. In this part of the exercise, we’ll develop an intuition for this concept:\n\n4.1.1.1 What is the likelihood?\nWhen discussing a particular distribution, we have playing with its associated random number generator function in R. For the binomial distribution, this is the rbinom() function. If you check the documentation for rbinom() though, with ?rbinom, you’ll see that you actually get documentation for 4 different functions, all with a similar naming convention. We’ll find use for all of these at some point in this class, but for now, let’s focus on one that we briefly visited last week, dbinom().\nUnlike rbinom(), dbinom() returns the same value for a given set of parameters every single time. It is not random:\n\ndbinom(50, 100, 0.5)\ndbinom(50, 100, 0.5)\n\n[1] 0.07958924\n[1] 0.07958924\n\n\nWhat does this value represent though. First, let’s understand its input:\n\nx: The number of “successes”. So essentially what you could get out from a given run of rbinom().\nsize: Same as in rbinom(), the number of trials.\nprob: Same as in rbinom(), the probability that each trial is a success.\n\nSo it seems to quantify something about a particular possible outcome of rbinom(). It assigns some number to an outcome given the parameters you could have used to simulate such an outcome. What does this number represent though? To find out, let’s plot it for a range of x’s:\n\nxs &lt;- 0:100\nds &lt;- dbinom(xs, 100, 0.5)\n\ntibble(x = xs,\n       d = ds) %&gt;%\n  ggplot(aes(x = x, y = d)) + \n  geom_point() + \n  theme_classic()\n\n\n\n\n\n\n\n\nValues closer to 50 get higher numbers than values further from 50, and the plot seems to be symmetric around 50. What’s significant about 50? It’s \\(size * prob\\), or the average value we would expect to get from rbinom() with these parameters!\nThis investigation should give you some sense that the output of dbinom() is in some way related to the probability of seeing x given a certain size and prob. Is it exactly this probability? We can gut check by assessing some cases we know for certain. Like, what is the probability of seeing 1 success in 1 trial if prob = 0.5? 0.5, because that’s the defintiion of prob! What does dbinom() give us in this scenario:\n\ndbinom(1, 1, 0.5)\n\n[1] 0.5\n\n\n0.5; that’s a good sign that we are on to something. Try out some different values of prob:\n\ndbinom(1, 1, 0.3)\n\n[1] 0.3\n\ndbinom(1, 1, 0.7)\n\n[1] 0.7\n\ndbinom(1, 1, 0.2315)\n\n[1] 0.2315\n\n\nEverything still checks out. How about the probability of 2 successes in 2 trials given a certain prob? Each trial has probability of prob of being a success, and each trial is independent. Therefore, the probability of 2 successes in 2 trials is \\(prob * prob\\). Does dbinom() give us the same output in that case?\n\n# Expecting 0.25\ndbinom(2, 2, 0.5)\n\n[1] 0.25\n\n# Expecting 0.01\ndbinom(2, 2, 0.1)\n\n[1] 0.01\n\n\nSure thing!\nConclusion: dbinom(x, size, prob), tells us the probability of seeing x successes in size trials given the probability of a success is prob. This is known as the “likelihood of our data”.\n\n\n4.1.1.2 Calculating the total likelihood\nIn the above examples, we took a single value of x, and passed it dbinom(). This told us the probability of seeing that one value given whatever parameter values we set. In most cases though, we typically have mulitple replicates of data. How can we calculate the likelihood of our entire dataset?\nI’ll pose a solution, and then demonstrate why the solution makes sense. The solution is to run dbinom() on each data point, and multiply the outcomes. This can be done like so:\n\ndata &lt;- c(48, 50, 49, 50)\nlikelihood &lt;- prod(dbinom(data, size = 100, prob = 0.5))\n\nWhy does this make sense? Consider a simple example where we flip a single coin twice. Say we see heads (H) then tails (T). What was the probability of seeing this if the coin had been fair? Well, consider the following geometric intuition for probability. Flipping a coin is like throwing a dart at a number line that ranges from 0 to 1. If that dart is equally likely to land anywhere on the number line, then we could define heads as that dart landing at a value of less than the probability of heads. This is because, a fraction x of the line lies to the left of x. Flipping a coin twice and getting heads then tails means having the dart land to the left of 0.5 once, and to the right of 0.5 once. We can visualize this as a square formed by our two lines. We are thus asking what the probability is that if we were to throw a dart at this square, that we land in the upper right quadrant (left of 0.5 on first throw, right of 0.5 on second). Well, what fraction of the total area does each quadrant take up? It’s whatever the probability of heads is (x) times the probability of tails (1-x). Thus, probabilities of seeing multiple data points multiplies.\nThere is one hidden assumption here, which is that of independence. We are assuming that the result of the first coin toss has no impact on the result of the second coin toss. This is an assumption ubiquitous throughout statistics, as it not only makes analyzing our data easier, in many cases meaningfully analyzing our data without this assumption would be impossible. At the same time though, its very important to be aware of this assumption, as there are lots of situations in which it can be seriously violated.\nConclusion: If each of a collection of N datapoints is “independent”, then the total likelihood of the dataset is equal to the product of the individual datapoint likelihoods. Datapoints being independent means that the value of any given datapoint does not influence the values of any other datapoint.\nCOMPUTATIONAL ASIDE\nAbove, we calculate the total likelihood. One problem that we will often run into with this calculation is that the product of all of the individual data point likelihoods will be really small for some potential parameter values. In this case, our computer might just assign these a value of 0, even though the true likelihood is non-zero. This is called an “overflow error”, and can mess with some of the things we do later in this course.\nFor this reason, we will instead calculate the “log-total-likelihood”. This is just the logarithm of the total likelihood. Why does this matter? All of the d&lt;distribution&gt;() functions have one additional parameter named log, that can be set to either TRUE or FALSE. Setting it to TRUE will lead to it outputting the logarithm of what it would otherwise output.\nIn addition, the log of a product is the same as the sum of logs:\n\nnumbers &lt;- rbinom(100, size = 100, prob = 0.5)\nll_one_way &lt;- log(prod(dbinom(numbers, 100, 0.5)))\nll_other_way &lt;- sum(dbinom(numbers, 100, 0.5, log = TRUE))\n\nll_one_way == ll_other_way\n\n[1] TRUE\n\n\nSo it is almost always preferable to sum the log-likelihoods of individual datapoints than it is to multiply their regular likelihoods.\n\n\n4.1.1.3 How can we use the total log-likelihood to fit models?\nThe total log-likelihood of our data turns out to be a great metric by which determine model fit. The idea is to find the parameter value that gives you the largest likelihood. That’s to say, we choose as our best fit the parameter value(s) that make our data as probabilistically reasonable as possible. This is known as the maximum likelihood estimates for our parameter(s). If we simulated data using this value, our simulated data would on average come closer to looking like our real data than it would for any other parameter values we could have simulated with.\nHow does this play out with our data? We can visualize the so-called log-likelihood function to see which value of prob maximizes it:\n\nprob_guesses &lt;- seq(from = 0, to = 1, by = 0.001)\nlog_likelihoods &lt;- sapply(prob_guesses, function(x) sum(dbinom(simdata, \n                                                           size = 100, \n                                                           prob = x, \n                                                           log = TRUE)))\n\nLdf &lt;- tibble(ll = log_likelihoods,\n              prob = prob_guesses)\n\nLdf %&gt;%\n  ggplot(aes(x = prob, y = ll)) +\n  geom_line() + \n  theme_classic()\n\n\n\n\n\n\n\n\nKeep in mind the y-axis is a log scale, so differences of 1 equate to an order of magnitude difference on the “regular” scale. Let’s zoom in around our putative values to get a better look at which value of prob is most likely:\n\nzoomed_plot &lt;- Ldf %&gt;%\n  filter(prob &gt;= 0.45 & prob &lt;= 0.55) %&gt;%\n  ggplot(aes(x = prob, y = exp(ll))) +\n  geom_line() + \n  theme_classic()\nzoomed_plot\n\n\n\n\n\n\n\n\nIt looks like a value of prob = 0.5 is not strictly speaking the best guess. Let’s see what value in our grid of prob_guesses gives the highest value, and annotate this value on our zoomed in plot:\n\nmax_ll &lt;- Ldf %&gt;% filter(ll == max(ll)) %&gt;%\n  dplyr::select(prob) %&gt;% unlist() %&gt;% unname()\nmax_ll\n\n[1] 0.497\n\nzoomed_plot +\n  geom_vline(xintercept = max_ll,\n             color = 'darkred')\n\n\n\n\n\n\n\n\nSo our best guess given this metric is a value for prob of around 0.497. That’s pretty darn close to the true value of 0.5!\n\n\n4.1.1.4 How we ACTUALLY find the maximum likelihood\nWhile the strategy for finding the maximum likelihood parameter estimate used above is ok for this toy example, we can do a lot better in terms of the speed and generalizability of our computational solution. If we can provide the function that we want to maximize (e.g., the log-likelihood), and the data needed to calculate the value of our function for a given parameter estimate, then R provides us with the tool necessary to find the parameter estimate that maximizes our function. That tool is the optim() function.\nHere is how you can use optim() to find the maximum likelihood estimate of prob in our example:\n\n### Step 1: Define the function to maximize\nbinom_ll &lt;- function(params, data, size = 100){\n  \n  # We estimate on a log scale for convenience so we convert here\n  prob &lt;- params[1]\n  \n  # Log-likelihood assuming binomial distribution\n  ll &lt;- dbinom(data, \n               size = size, \n               prob = prob, \n               log = TRUE)\n  \n  # Return negative because optim() by default will find the minimum\n  return(-sum(ll))\n  \n}\n\n\n### Step 2: run optim()\nfit &lt;- optim(\n  par = 0.5, # initial guess for prob\n  fn = binom_ll, # function to maximize\n  data = simdata, # something passed to the function\n  method = \"L-BFGS-B\", # Most efficient method for finding max,\n  lower = 0.001, # Lower bound for parameter estimate\n  upper = 0.999 # Upper bound for parameter estimate\n  )\n\n\n### Step 3: Inspect the estimate\nprob_est &lt;- fit$par[1]\nprob_est\n\n[1] 0.4969\n\n\nThis is very close to what we found through other means, but it has the distinction of being the most accurate, automated method we have devised. In fact, this method, known as numerical estimation of the maximum likelihood parameter(s), is a mainstay in modern statistics.\nEND OF LECTURE 1\n\n\n\n4.1.2 Quantifying uncertainty\nThe key problem that we set up in both this chapter and the last is one of navigating the randomness in our data. We have to analyze our data in a way that is cognizant of this randomness. This means not only estimating things we care about, but also being honest about how confident we are in our estimates.\nIn the binomial distribution fitting exercise, we arrived at a strategy for finding what we considered the “best” estimate for the prob parameter. On that journey though, you may have gotten the sense that there wasn’t one definitive parameter estimate. Rather, a range of estimates would have been reasonable given the data. You may have also realized that we didn’t fully stick the landing. Our estimate was not exactly equal to the true value of 0.5. It would thus be best if we didn’t only report a single number as our best estimate. We need to also provide some sense of how confident we are that our estimate is right. In this section, we will discuss some of the most common ways to do this.\n\n4.1.2.1 The bootstrap\nThe boostrap is one of the cutest, somehow legitimate ideas in statistics that is a shockingly accurate way by which to assess uncertainty in your parameter estimates. The idea is to take your original dataset and sample from it with replacement. This means creating a new dataset, the same size of the original dataset, which is generated by randomly choosing datpoints from the original dataset until you have chosen N datapoints (N being the number of datapoints in your original dataset). Sampling “with replacement” means that every time you sample a datapoint, you “put it back”, so to speak. So each sample is taken from the full original dataset. This ensures that the new dataset you generate has almost no chance of looking exactly like your original dataset. You can then re-estimate the maximum likelihood parameter with each new re-sampled dataset, and see how much variability there is from re-sampling to re-sampling. From this, you can derive measures of uncertainty, like the standard devitaion of your resampled data estimates.\nLet’s put this into action with our simulated data:\n\nnresamps &lt;- 500\nestimates &lt;- rep(0, times = nresamps)\n\nfor(i in 1:nresamps){\n  \n  ### Step 1: resample\n  resampled_data &lt;- sample(simdata, \n                           size = length(simdata),\n                           replace = TRUE)\n  \n  \n  ### Step 2: estimate\n  fit &lt;- optim(\n    par = 0.5, # initial guess for prob\n    fn = binom_ll, # function to maximize\n    data = resampled_data, # something passed to the function\n    method = \"L-BFGS-B\", # Most efficient method for finding max,\n    lower = 0.001, # Lower bound for parameter estimate\n    upper = 0.999 # Upper bound for parameter estimate\n    )\n  \n  \n  ### Step 3: save estimate\n  estimates[i] &lt;- fit$par[1]\n  \n  \n}\n\n### Uncertainty\nsd(estimates)\n\n[1] 0.004929947\n\n### Check out distribution of estimates\ntibble(estimate = estimates) %&gt;%\n  ggplot(aes(x = estimate)) + \n  geom_histogram(color = 'black',\n                 fill = 'darkgray',\n                 bins = 25) + \n  theme_classic() + \n  xlab(\"Estimate\") + \n  ylab(\"Count\") + \n  xlim(c(0.45, 0.55))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n4.1.2.1.1 Exercise:\nRerun this bootstrapping multiple times (e.g., 5) for 3 different values of nresamp. How does the size of nresamp relate to the output.\n\n\n\n4.1.2.2 The shape of your likelihood\nBootstrapping is easy, intuitive, and great. It is not without its downsides though. For one, it is often computationally intensive. We are forcing ourselves to rerun our analysis N times, where N is our chosen number of bootstrap samples. If our dataset is large and/or our model complex (i.e., there are lots of parameters to estimate), this can take our computer quite a long time. In addition, as this method relies on simulation, it is prone to variation. That is to say, from bootstrap to bootstrap, your estimate for your uncertainty will vary. You can decrease the amount of variance by increasing N, but that comes with greater comptuational cost. It would be awesome if we could somehow extract information about our uncertainty directly from our data and model.\nSuch a method exists! The premise is that we can derive a measure of uncertainty from the shape of our likelihood function. Let’s see what that means by exploring two cases, one in which we have lots of data and one in which we have very little data.\n\n# Simulate small and large datasets\nsmall_data &lt;- rbinom(10, size = 100, prob = 0.5)\nbig_data &lt;- rbinom(100, size = 100, prob = 0.5)\n\n\n# Calculate log-likelihoods\nprob_guesses &lt;- seq(from = 0, to = 1, by = 0.001)\nsmall_log_likelihoods &lt;- sapply(prob_guesses, function(x) sum(dbinom(small_data, \n                                                           size = 100, \n                                                           prob = x, \n                                                           log = TRUE)))\nbig_log_likelihoods &lt;- sapply(prob_guesses, function(x) sum(dbinom(big_data, \n                                                           size = 100, \n                                                           prob = x, \n                                                           log = TRUE)))\n\n# Plot them\nLdf &lt;- tibble(ll = c(small_log_likelihoods,\n                     big_log_likelihoods),\n              size = factor(rep(c(\"small\", \"big\"),\n                                each = length(prob_guesses))),\n              prob = rep(prob_guesses, times = 2))\n\nLdf %&gt;%\n  dplyr::filter(size == \"small\") %&gt;%\n  ggplot(aes(x = prob, y = exp(ll))) +\n  geom_line() + \n  theme_classic() +\n  coord_cartesian(xlim = c(0.4, 0.6))\n\n\n\n\n\n\n\nLdf %&gt;%\n  dplyr::filter(size == \"big\") %&gt;%\n  ggplot(aes(x = prob, y = exp(ll))) +\n  geom_line() + \n  theme_classic() +\n  coord_cartesian(xlim = c(0.4, 0.6))\n\n\n\n\n\n\n\n\nWhat do you notice about the two likelihood function curves? The small dataset curve is broader than the large dataset curve. Why is this the case? Because with less data, it’s more difficult to distinguish between different parameter values. Seeing 5 heads in 10 flips isn’t all that crazy for a coin with a probability of heads of 0.6 or 0.4. Seeing 50 heads in 100 flips though with that kind of coin is a lot less likely. Collecting more data narrows the space of reasonable parameters.\nThus, intuitively, the sharpness of the likelihood function about its maximum could be a reasonable estimate for our uncertainty. Formalizing this idea takes a good deal of math, but the idea is that calculus provides us with a quantity that describes how sharp a function peaks about its maximum: the second derivative. The second derivative of our likelihood function can be provided by optim() in the form of what is known as the Hessian:\n\n### Step 1: run optim()\nfit &lt;- optim(\n  par = 0.5, # initial guess for prob\n  fn = binom_ll, # function to maximize\n  data = simdata, # something passed to the function\n  method = \"L-BFGS-B\", # Most efficient method for finding max,\n  lower = 0.001, # Lower bound for parameter estimate\n  upper = 0.999, # Upper bound for parameter estimate\n  hessian = TRUE # Return the Hessian\n  )\n\n### Estimate uncertainty\nuncertainty &lt;- sqrt(solve(fit$hessian))\n\n\nuncertainty\n\n            [,1]\n[1,] 0.004999884\n\n\nCompare this value with the one we got through bootstrapping. They’re pretty similar!\n\n\n4.1.2.3 Uncertainty in “higher dimensions”\nEND OF LECTURE 2\n\n\n\n4.1.3 A self-guided exercise: fit a negative binomial\nLet’s make this more RNA-seq relevant now. As we will talk about more throughout this course, one of the most common models for the replicate-to-replicate variability of RNA-seq read counts is a negative binomial distribution. In this case, there are two parameters to estimate. In R, they are referred to as mu and prob. Simulate some data with a given value for these two parameters, and follow the steps above to find a strategy that consistently returns accurate estimates for the parameter values you simulated with.\n\n\n4.1.4 A 2nd self-guided exercise: fit a normal distribution\nEND OF LECTURE 3",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Statistical Modeling</span>"
    ]
  },
  {
    "objectID": "modeling.html#problem-set",
    "href": "modeling.html#problem-set",
    "title": "4  Introduction to Statistical Modeling",
    "section": "4.2 Problem set",
    "text": "4.2 Problem set\n\n4.2.1 Fitting the wrong model\nExplore and document what happens when you simulate using one distribution but fit a normal distribution to the data. How do the mean and standard deviation of your data relate to the estimated mean and sd parameters of the normal distribution? Do this with two non-normal distributions of your choice.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Statistical Modeling</span>"
    ]
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "5.1 Does your data support your hypothesis?\nAll scientific endeavors can be boiled down to two steps:\nMaybe you hypothesize that molecule X is a biomarker for a disease (i.e., its presence in an individual suggests that individual has the disease). You could measure the levels of molecule X in healthy and diseased patients to test your hypothesis. Maybe you hypothesize that transcript Y gets downregulated when you treat cells with drug Z. In any case, data that will provide evidence for or against said hypothesis needs to be collected.\nThere is a third step which at first may feel like a trivial extension of the second: analyzing your data and drawing a conclusion about your hypothesis. In practice though, this represents an immense challenge. You may have encountered an algorithm by which to solve this problem, known as null-hypothesis statistical testing. You may have been taught how to calculate p-values and how to draw conclusions from these p-values. Despite the ubiquitous algorithmization of this process, it has come under increased, heated scrutiny. Some have even accused it of launching a widespread reproducibility crisis plaguing the entire scientific enterprise, and have called for the death of this long extant practice.\nWhat are these p-values and are they really that evil? To answer this question, we are going to explore the question that titles this section: “Does your data support your hypothesis?”. We are going to journey to the depths of this question to understand the challenges it poses and the ways we can go about tackling it. During this exploration, we will discover null-hypothesis significance testing (NHST). What we will find though, is that NHST is not the definitive solution to our quandery. It is a cheap hack that offers us an easy way out at the cost of falling short of our original goal. Despite the imperfections of its solution, it is not without merit though, and we will attempt to understand why it’s ok to give up sometimes.",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#classic-example-the-fair-coin",
    "href": "hypothesis.html#classic-example-the-fair-coin",
    "title": "5  Hypothesis Testing",
    "section": "6.1 Classic example: the fair coin",
    "text": "6.1 Classic example: the fair coin\nImagine you have a coin, and you would like to test the hypothesis, “is this coin fair?”. A “fair” coin is one that when flipped, is equally likely to come up heads or tails.\nWe can cast this question in the language of statistical modeling. If we assume that each coin flip is independent (e.g., the last outcome does not influence the next) and that the probability of heads is unchanged from flip to flip, then we can model the number of heads as following a binomial distribution. We know how many times we flip the coin, so there is one unknown parameter, prob, which represents the probability of seeing a head. A fair coin is one in which prob = 0.5. In this framing, our task is to determine whether or not the “true prob” for our coin is exactly 0.5.\nHow should we go about answering this question?\n\n6.1.1 Step 1: Collect some data\nThe only way we can begin to tell if the coin is fair or not is to flip it!",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#z-test",
    "href": "hypothesis.html#z-test",
    "title": "5  Hypothesis Testing",
    "section": "6.1 Z-test",
    "text": "6.1 Z-test",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#t-test",
    "href": "hypothesis.html#t-test",
    "title": "5  Hypothesis Testing",
    "section": "6.2 T-test",
    "text": "6.2 T-test",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#anova",
    "href": "hypothesis.html#anova",
    "title": "5  Hypothesis Testing",
    "section": "6.3 ANOVA",
    "text": "6.3 ANOVA",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#non-parametric-tests",
    "href": "hypothesis.html#non-parametric-tests",
    "title": "5  Hypothesis Testing",
    "section": "6.4 Non-parametric tests",
    "text": "6.4 Non-parametric tests",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#custom-nshting",
    "href": "hypothesis.html#custom-nshting",
    "title": "5  Hypothesis Testing",
    "section": "6.5 Custom NSHTing",
    "text": "6.5 Custom NSHTing",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#multiple-test-adjustment",
    "href": "hypothesis.html#multiple-test-adjustment",
    "title": "5  Hypothesis Testing",
    "section": "7.1 Multiple-test adjustment",
    "text": "7.1 Multiple-test adjustment",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#power",
    "href": "hypothesis.html#power",
    "title": "5  Hypothesis Testing",
    "section": "7.2 Power",
    "text": "7.2 Power",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#miscalibration",
    "href": "hypothesis.html#miscalibration",
    "title": "5  Hypothesis Testing",
    "section": "7.3 Miscalibration",
    "text": "7.3 Miscalibration",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lm.html",
    "href": "lm.html",
    "title": "6  Linear Modeling",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Modeling</span>"
    ]
  },
  {
    "objectID": "dreduce.html",
    "href": "dreduce.html",
    "title": "7  Dimensionality Reduction",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "cluster.html",
    "href": "cluster.html",
    "title": "8  Clustering and Mixture Modeling",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clustering and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "bioinfo.html",
    "href": "bioinfo.html",
    "title": "9  Practical Bioinformatics",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Statistics in the Wild",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Practical Bioinformatics</span>"
    ]
  },
  {
    "objectID": "finalproject.html",
    "href": "finalproject.html",
    "title": "10  Analysis of an RNA-seq dataset",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Statistics in the Wild",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis of an RNA-seq dataset</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "hypothesis.html#does-your-data-support-your-hypothesis",
    "href": "hypothesis.html#does-your-data-support-your-hypothesis",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "Coming up with a hypothesis about how some aspect of the world works.\nCollecting data to test said hypothesis.",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#the-toy-example-is-this-coin-fair",
    "href": "hypothesis.html#the-toy-example-is-this-coin-fair",
    "title": "5  Hypothesis Testing",
    "section": "5.2 The toy example: is this coin fair?",
    "text": "5.2 The toy example: is this coin fair?\nIt’s always best to start simple and understand our problem in the context of a toy example. An almost cliche testing ground for hypothesis testing is determining whether or not a coin is fair. Despite being overused, it will serve us well. The scenario is that you are given a coin and asked, “is this coin fair?”. A fair coin is one that when flipped, has an equal probability of showing up heads or tails.\nTake a second to stop and think about how you would try to answer this question.\nStep 1 has to be to collect some data. How can we know if the coin is fair if we have never seen it flipped? The path towards answering any scientific question starts with data collection. As a part of this toy example, it will be useful to work with simulated data. The great thing about simulated data is that we know everything there is to know about how the data was created. We even know the answer to any question we could hope to ask about said data. In this case, we will “flip” a hypothetical coin 10 times and print the result. You can check out the code if you want to see what this simulation looks like. You can also check out the code if you want to see if this simulated coin is fair or not (how would you tell?):\n\n5.2.0.1 Exercise 1: simulate 10 coin flips\n\n\nShow the code\n# Set seed to ensure consistent results\nset.seed(164)\n\n# Flip coins 10 times\nflips &lt;- rbinom(10, size = 1, prob = 0.5)\n\n# Convert to strings of heads (1s) and tails (0s):\nflips_HT &lt;- ifelse(flips == 1, \"H\", \"T\")\n\n# Show results\ncat(\"10 flips of the coin yielded: \", flips_HT, \"\\n\")\n\n\n10 flips of the coin yielded:  H T H H H H T H H T \n\n\nSo in my simulation, I saw 7 heads and 3 tails.\nTake some time to think about whether this is this strong evidence for or against fairness. How would you answer this question?",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#assessing-a-coins-fairness",
    "href": "hypothesis.html#assessing-a-coins-fairness",
    "title": "5  Hypothesis Testing",
    "section": "5.3 Assessing a coin’s fairness",
    "text": "5.3 Assessing a coin’s fairness\nWe have now collected data about our coin. Step 2 is we need to find a way to analyze this data so as to assess whether or not our hypothesis of fairness is supported by the data. To help you arrive at an idea for how to do this, I will pose and discuss answers to a set of questions:\n\n5.3.1 Can you definitively answer the question?\nIn other words, would it be possible to conclusively state “yes this coin is fair” or “no this coin is unfair” with complete certainty? Put another way, what would need to be true about flipping coins for this to be possible?\nI would argue for this to be the case, we would need a certain kind of behavior from our coin. We would need the behavior of fair coins to always be distinct from that of unfair coins. For example, if fair coins always come up heads between 4 and 6 times when flipped 10 times, and a coin with any amount of unfairness always comes up head &lt; 4 times (if it’s biased towards tails) or &gt; 6 times (if it’s biased towards heads), then we’re set. If we see 4, 5, or 6 heads, we could automatically conclude that the coin is fair. If we see 1, 2, 3, 7, 8, 9, or 10 heads, we could automatically conclude that the coin is unfair. Any overlap in the possible outcomes of fair and unfair coins would mean that if faced with data in this realm of ambiguity, we would have no choice but to concede that we are not 100% certain that the coin is fair or unfair.\nYour intuition is probably telling you that this would be absurd behavior for a coin to follow. What if a coin is only a little bit biased? Let’s say it comes up heads 51% of the time. How crazy would it be for a such a coin to come up heads 4, 5, or 6 out of 10 flips? And on the other side, would you really be that blown away if a fair coin came up heads 7 times?\nConclusion: We can only answer the fairness question probabilistically\nThat is to say, the best we can hope to do is to determine a probability that the hypothesis of fairness is true. This fact holds true for almost all scientific inquiries.\n\n\n5.3.2 How can we describe the probability of a given outcome of our experiment?\nWe need to develop a statistical model for our data, like we discussed in the last unit. This means laying out assumptions for how our data is generated, and using those assumptions to figure out distributions that accuratley describe the variability in our data.\n\n5.3.2.1 Exercise 2: What assumptions did you make in your simulations?\nIn our case, it would be reasonable to assume the following things about your data:\n\nEach flip of the coin is independent. This means that the probability of seeing a heads on the next flip is unaffected by what you saw on any of your previous flips.\nEvery flip has the same probability of coming up heads. Let’s call this probability PH.\n\n\n\n5.3.2.2 Exercise 3: What distribution do you expect such data to follow?\nThis process of independent trials with two possible outcomes feels a lot like a binomially distributed random variable. Thus, we can use the relevant proability mass function (dbinom()) to assign probabilites to various outcomes. For example, give me a value of PH and N, and I can tell you the probability of seeing any number of heads, H. Here’s what these probabilities look like for a fair coin flipped 10 times:\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\n# Number of flips\nN &lt;- 10\n\n# Probability of heads\nPH &lt;- 0.5\n\n# Number of heads\nHs &lt;- 0:N\n\n\n# P(H | N, PH)\npofH &lt;- round(dbinom(Hs, N, prob = PH),\n              digits = 3)\n\ntibble(pofH = pofH,\n       H = factor(Hs)) %&gt;%\n  ggplot(aes(x = H, y = pofH)) + \n  geom_bar(stat = 'identity') + \n  theme_classic() + \n  xlab('H') + \n  ylab('Pr(H)') + \n  ggtitle(paste0(\"PH = \", PH)) +\n  geom_text(aes(label = pofH, x = H, y = pofH), \n            vjust = -0.6)\n\n\n\n\n\n\n\n\n\nAre these probabilities surprising to you?\nSome observations:\n\nThe most likely result for a fair coin is 50% heads and 50% tails.\nThe plot is symmetric about the 50/50 result. Seeing 6 heads is as likely as seeing 4 heads, seeing 7 heads is as likely as seeing 3 heads, etc.\nAll feasible results (0-10 flips) have some probability of occurring. This confirms our suspicion that definitively distinguishing fair and unfair coins is off the table.\nWhile seeing 5 heads is the most likely outcome, it isn’t mind-blowingly more likely than seeing 4 or 6 heads. In fact, the probability of being 1 off of perfectly even (i.e., seeing 4 OR 6 heads) is more likely than seeing exactly 5 head. 4 and 6 heads each have probability of ~0.205, making the probability of one of these events occurring ~0.410. Compare that to the probability of 5 heads, ~0.246.\n\nWhat does this plot look like for an unfair coin? Let’s set PH to 0.7 and see what happens:\n\n\nShow the code\n# Probability of heads\nPH &lt;- 0.7\n\n# P(H | N, PH)\n  # More precision to prevent 0s\npofH &lt;- round(dbinom(Hs, N, prob = PH),\n              digits = 5)\n\n\ntibble(pofH = pofH,\n       H = factor(Hs)) %&gt;%\n  ggplot(aes(x = H, y = pofH)) + \n  geom_bar(stat = 'identity') + \n  theme_classic() + \n  xlab('H') + \n  ylab('Pr(H)') + \n  ggtitle(paste0(\"PH = \", PH)) +\n  geom_text(aes(label = pofH, x = H, y = pofH), \n            vjust = -0.6)\n\n\n\n\n\n\n\n\n\nCompare and contrast that to the fair coin plot\nTo help with comparing, let’s overlap the two:\n\n\nShow the code\n# Probability of heads\nPH1 &lt;- 0.7\nPH2 &lt;- 0.5\n\n# P(H | N, PH)\n  # More precision to prevent 0s\npofH1 &lt;- dbinom(Hs, N, prob = PH1)\npofH2 &lt;- dbinom(Hs, N, prob = PH2)\n\n\ntibble(pofH = c(pofH1, pofH2),\n       H = factor(c(Hs, Hs)),\n       PH = factor(rep(c(PH1, PH2), each = 11))) %&gt;%\n  ggplot(aes(x = H, y = pofH, \n             fill = PH, color = PH)) + \n  geom_bar(stat = 'identity',\n           position = 'identity',\n           alpha = 0.1) + \n  theme_classic() + \n  scale_fill_manual(values = c('chartreuse3',\n                               'darkslateblue')) +\n  scale_color_manual(values = c('chartreuse3',\n                               'darkslateblue')) +\n  xlab('H') + \n  ylab('Pr(H)')\n\n\n\n\n\n\n\n\n\nObservations:\n\nThe most likely result is 7 heads and 3 tails. Do you notice a pattern? In short, its no coincidence that the most likely result equates to the product of N and PH. Try setting PH to something such that PH * N is not an exact integer. Guess what number of heads will be most likely and check your intuition.\nThe distribution is no longer symmetric. The probabilities seem to be bunching up due to there being a hard upper bound of “number of flips”.\n\n\n\n\n5.3.3 How can we determine the probability of fairness?\nWith a probability function in hand, we can now begin tackling our main challenge: determining the probability our coin is fair.\n\n5.3.3.1 Trying the easy thing\nWe want to know how likely our hypothesis of a fair coin is given the data we collected. We have at our disposal a probability function for our data. The easiest thing you could think to do is plug in our data and hypothesized value of PH into this formula and take the probability we get out as the probability our hypothesis is true. For 7 heads in 10 flips we get:\n\n\nShow the code\n# Show results\ncat(\"Pr(7 H | PH = 0.5): \", round(dbinom(7, 10, 0.5), 3), \"\\n\")\n\n\nPr(7 H | PH = 0.5):  0.117 \n\n\nThis seems like a reasonable ballpark. 7 heads seems a bit weird for a fair coin, and we saw a string of 4 straight heads in our dataset, which is surprising. At the same time, 10 flips isn’t much, and so a couple extra heads isn’t crazy. An 11.7% chance that our hypothesis is correct kinda feels right.\nWhenever we think we have an answer, its best to scrutinize it like a skeptic. What if we had flipped our coin a lot more times? Say we flipped it 100 times instead of 10. If we had gotten exactly 50 heads in this case, how confident should we be that our coin is fair? More or less than in our 7 heads in 10 flips result?\nIntuitively, 50 heads in 100 flips is a lot better evidence in favor of fairness than 7 heads in 10 flips. It’s more data and exactly 50% heads, vs. 70% heads. So if we think our metric is accurately measuring the probability that our coin is fair, it should be higher in the 50/100 case. Let’s check and plug 50 heads in 100 flips to our probability function:\n\n\nShow the code\n# Show results\ncat(\"Pr(50 H | PH = 0.5, N = 100): \", round(dbinom(50, 100, 0.5), 3), \"\\n\")\n\n\nPr(50 H | PH = 0.5, N = 100):  0.08 \n\n\nWoah, that’s lower than the 7 heads in 10 flips case! How about if we got exactly 500 heads in 1000 flips\n\n\nShow the code\n# Show results\ncat(\"Pr(509 H | PH = 0.5, N = 1000): \", round(dbinom(500, 1000, 0.5), 3), \"\\n\")\n\n\nPr(509 H | PH = 0.5, N = 1000):  0.025 \n\n\nIt’s even lower!! That’s evidence that our strategy is missing something. What’s going on?\nIt might be helpful to go back to our full probability function. What does it look like for a fair coin vs one possible unfair coin, each flipped 100 times?\n\n\nShow the code\n# Probability of heads\nPH1 &lt;- 0.7\nPH2 &lt;- 0.5\n\n# P(H | N, PH)\n  # More precision to prevent 0s\npofH1 &lt;- dbinom(0:100, 100, prob = PH1)\npofH2 &lt;- dbinom(0:100, 100, prob = PH2)\n\n\ntibble(pofH = c(pofH1, pofH2),\n       H = factor(c(0:100, 0:100)),\n       PH = factor(rep(c(PH1, PH2), each = 101))) %&gt;%\n  ggplot(aes(x = H, y = pofH, \n             fill = PH, color = PH)) + \n  geom_bar(stat = 'identity',\n           position = 'identity',\n           alpha = 0.1) + \n  theme_classic() + \n  scale_fill_manual(values = c('chartreuse3',\n                               'darkslateblue')) +\n  scale_color_manual(values = c('chartreuse3',\n                               'darkslateblue')) +\n  xlab('H') + \n  ylab('Pr(H)')\n\n\n\n\n\n\n\n\n\nAs we saw before, 50 heads and 50 tails is the most likely outcome for a fair coin. The problem though is that the more we flip the coin, the more possible outcomes there are. With 100 flips, everything from 40-60 heads is pretty commonly seen with a fair coin. Thus, the probability of any one of these events gets smaller and smaller with more flips.\n\n\n5.3.3.2 Why your coin isn’t fair\nOur first attempt forces us to reckon with something about our question of “what is the probability that our coin is fair”. We have built a model for coin flipping and decided that a fair coin is one for which a parameter in this model (PH) takes on a precise value (0.5). Does it make sense to ask such an exact question though? Can PH ever be expected to be exactly 0.5 for any coin?\nThe point I am hinting at here is a subtlety of probability theory that trips many students up. The probability of heads for our coin should be thought of as a continuous random variable. It can take on any value between 0 and 1 on the real number line. When discussing such random variables, it doesn’t make much sense to talk about the probability of any particular value. I’ll point you to a great 3blue1brown YouTube video for more context (https://www.youtube.com/watch?v=ZA4JkHKZM50), but in short, when there are an infinite number of things that could happen (more precisely an uncountably infinite), the probability of any particular outcome is exactly 0. Thus, we can only talk about the probability that our outcome falls within a range of outcomes (e.g., heads probability between 0.45 and 0.55).\nTherefore, your coin isn’t fair. The probability that it is exactly fair is exactly 0. One way to navigate this challenge is to change the problem specification a bit. Instead of asking whether or not the coin is exactly fair, we need to consider a range of heads probabilities. If we have reason to believe that our coin’s probability of coming up heads falls outside of this range, then we’ll say the coin is unfair. Otherwise, we will call it fair.\n\n\n5.3.3.3 Idea #2: Integrating the likelihood function\nIf it’s a range of values we need to consider, why not just sum of the value for idea #1 for a range of potential PHs? We could then look at this probability as a function of the range of PHs we choose and come to decisions about coin fairness from that.\n\n### Parameters to set\n\n# Number of heads\nH &lt;- 7\n\n# Number of flips\nN &lt;- 10\n\n# Lower bound on PH range to consider\nlower_bound &lt;- 0.45\n\n# Upper bound on PH range to consider\nupper_bound &lt;- 0.55\n\n# Integration rectangle width\ndPH &lt;- 0.00001\n\n### Calculate\n\ncalc_integral &lt;- function(H = 5, N = 10, \n                          lower_bound = 0.45, upper_bound = 0.55,\n                          dPH = 0.00001){\n  \n  PHstosum &lt;- seq(from = lower_bound, to = upper_bound,\n                by = dPH)\n  output &lt;- 0\n  \n  for(p in seq_along(PHstosum)){\n    \n    output &lt;- output + dbinom(H, N, PHstosum[p])*dPH\n    \n  }\n  \n  return(output)\n  \n}\n\n\noutcome &lt;- calc_integral(H=H, N=N, lower_bound=lower_bound,\n              upper_bound=upper_bound, dPH=dPH)\n\ncat(\"Result is: \", round(outcome, 3), \"\\n\")\n\nResult is:  0.012 \n\n\n\n\n5.3.3.4 Considering prior experiences/knowledge\n\n\n5.3.3.5 Why the true answer is tough\n\n\n5.3.3.6 The easy way out: NHST and p-values\n\n\n5.3.3.7 p-values vs the actual solution\n\n\n5.3.3.8 Can you just plug and chug? Why Pr(data | PH = 0.5) ain’t it\n\n\n\n\n\n\nAn intuition for what went wrong\n\n\n\n\n\n\n\n\n\n\n\n5.3.3.9 Considering all possible hypotheses\nDespite the flaws of Pr(H | PH = 0.5), it is on the right track. Look again at the comparison of 10 and 100 flips of a fair and 70% heads coin (figures X and Y). Focus your attention on the 50/50 bar (5 heads out of 10 or 50 heads out of 100). Compare the heights of this bar for the fair and unfair coin. What is the impact of more flips?\nWhile the absolute height of the bar for the fair coin does not tell us how likely our coin is to be fair, the relative bar heights for the fair and unfair coins seem to trend in the right direction. We are a little less than half as likely to see 5 heads in 10 flips with the 70% heads biased coin as we are with the fair coin. In 100 flips though, seeing 50 heads is WAY more likely with a fair coin than with the 70% heads coin.\nConclusion: P(H | PH = 0.5) does not matter on its own, what matters is its size relative to all other possible hypotheses.\nThat’s to say, we need to calculate:\n\\[\n\\frac{\\text{P(H | PH = 0.5)}}{\\int_{0}^{1} \\text{P(H | PH = x)}\\,dx}\n\\] Integrals can be a bit of a pain in the butt to calculate. That’s part of what will motivate us to look for an easier way out. For now though, let’s see what various outcomes for 10 flips yields for this new quantity:\n\n\nShow the code\n# Step size\ndph &lt;- 0.001\n\n# phs to calculate value at for numerical integration\nPH_grid &lt;- seq(from = dph/2, to = 1 - dph/2, by = dph)\nsubgrid &lt;- PH_grid[PH_grid &gt; 0.45 & PH_grid &lt; 0.55]\n\nHs &lt;- 0:10\n\nPofFair &lt;- rep(0, times = length(Hs))\n\nfor(h in seq_along(Hs)){\n  \n  num &lt;- sum(dbinom(Hs[h], size = 10, prob = subgrid)*dph)\n  denom &lt;- sum(dbinom(Hs[h], size = 10, prob = PH_grid)*dph)\n  \n  PofFair[h] &lt;- num/denom\n}\n\ntibble(PofFair = PofFair,\n       H = factor(Hs)) %&gt;%\n  ggplot(aes(x = H, y = PofFair)) +\n  geom_bar(stat = \"identity\") + \n  theme_classic() + \n  xlab(\"H\") + \n  ylab(\"Fairness probability (2nd try)\")\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.4 Why I still use p-values\nUnique aspects of my use case:\n\nStatistical pragmatism\nEDA and list ordering\nOrthogonal validation\nMultiple testing",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  }
]