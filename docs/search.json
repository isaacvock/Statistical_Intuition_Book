[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Intuition for Modern Biologists",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "rnaseq.html",
    "href": "rnaseq.html",
    "title": "1  Introduction to RNA-seq",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to RNA-seq</span>"
    ]
  },
  {
    "objectID": "intro2R.html",
    "href": "intro2R.html",
    "title": "2  Introduction to R",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "3  Introduction to Probability",
    "section": "",
    "text": "4 The Many Flavors of Randomness\nTo understand how statisticians think about randomness, consider an analogy. In biology, everything is complicated and every system we study seems unique. To tackle this complexity, we group things into bins based on important characteristics they share, and then we try to understand the patterns that govern each bin. Bins in biology may be determined by the type of organism being studied (e.g., entymology, mammology, microbiology, etc.), the activities of molecular machines you investigate (e.g., phosphorylating subtrates, regulating transcription, intracellular transport, etc.), and much more. Similarly in statistics, every kind of data seems to possess unique sources and types of variance. Work with enough data though and you will notice that there are patterns in the types of randomness we commonly see. We thus focus on understanding these common patterns and apply them to understanding our unique data.\nThe patterns in randomness that we observe are coined “probability distributions”. Think of them as functions, which take as input a number, and provide as output the probability of observing that number. These functions could take any shape you dream up, but as mentioned, particular patterns crop up all of the time. In the following exercises, we are going to explore these patterns and understand what gives rise to them.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#the-basics",
    "href": "probability.html#the-basics",
    "title": "3  Introduction to Probability",
    "section": "4.1 The basics",
    "text": "4.1 The basics\nTo illustrate some general facts about randomness, we will start by working with two functions in R: rbinom() and rnorm(). These are two of many “random number generators” (RNGs) in R. As we will learn in these exercises, there isn’t just one kind of randomness, so there isn’t just one RNG.\nrbinom and rnorm both have 3 parameters. Both have a parameter called n, which sets the number of random numbers to generate. rbinom has two other parameters called size and prob. size can be any integer &gt;= 0. prob can be any number between 0 and 1 (including 0 and 1). rnorm’s two additional parameters are called mean and sd. mean can be any number, and sd can be any positive number. Take some time to play with both of these functions. As you do so, consider the following:\n\nIn what ways are their output similar?\nIn what ways are their output distinct?\nHow do prob and size impact the output of rbinom?\nHow do mean and sd impact the output of rnorm?\nMake histograms with different numbers of random numbers (i.e., different n). What do you see as n increases? Now repeat this but with different parameters for the relevant function. What changes?\n\n\n4.1.1 Example exercise:\nGenerated 5 draws from each to compare general output:\n\nprint(\"rbinom output:\")\n\n[1] \"rbinom output:\"\n\nrbinom(10, size = 100, prob = 0.5)\n\n [1] 49 52 51 54 50 46 35 45 44 47\n\nprint(\"rnorm output:\")\n\n[1] \"rnorm output:\"\n\nrnorm(10)\n\n [1] -0.8945617 -0.4140846 -0.6287162 -0.8193058 -2.1102337  1.3816885\n [7] -1.4874796 -1.6302216  0.3103661 -0.1578150\n\n\nObservations:\n\nrbinom seems to only output exact integers. rnorm’s output has lots of decimal places\nAs expected, output for both usually differs from run to run\nrnorm output seems to always differ while rbinom might spit out the same number from time to time.\n\nTaking more draws reveals patterns in the randomness that is not as evident from less draws:\nb5 &lt;- rbinom(n = 5, size = 100, prob = 0.5)\nb50 &lt;- rbinom(n = 10, size = 100, prob = 0.5)\nb500 &lt;- rbinom(n = 100, size = 100, prob = 0.5)\nb5000 &lt;- rbinom(n = 1000, size = 100, prob = 0.5)\n\n\nhist(b5, main = \"5 draws\", xlab = \"Random number\")\nhist(b50, main = \"10 draws\", xlab = \"Random number\")\nhist(b500, main = \"100 draws\", xlab = \"Random number\")\nhist(b5000, main = \"1000 draws\", xlab = \"Random number\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVarying size of rbinom:\nb100 &lt;- rbinom(500, size = 100, prob = 0.5)\nb200 &lt;- rbinom(500, size = 200, prob = 0.5)\nb300 &lt;- rbinom(500, size = 300, prob = 0.5)\n\nhist(b100, main = \"size = 100\", xlab = \"Random number\")\nhist(b200, main = \"size = 200\", xlab = \"Random number\")\nhist(b300, main = \"size = 300\", xlab = \"Random number\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher size generates larger random numbers. In fact, distribution of random numbers seems to increase in proportion to increase in size (i.e., doubling size doubles the average random number generated)\nVarying prob of rbinom\nb0.3 &lt;- rbinom(500, size = 100, prob = 0.3)\nb0.5 &lt;- rbinom(500, size = 200, prob = 0.5)\nb0.7 &lt;- rbinom(500, size = 300, prob = 0.7)\n\nhist(b0.3, main = \"prob = 0.3\", xlab = \"Random number\")\nhist(b0.5, main = \"prob = 0.5\", xlab = \"Random number\")\nhist(b0.7, main = \"prob = 0.7\", xlab = \"Random number\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigher prob has a similar effect to higher size, increasing the average magnitude of random numbers generated. There also seems to be a similar linear proportionality.\n\n\n4.1.2 Takeaways\n\nThere are two kinds of randomness: discrete and continuous.\nDiscrete randomness is randomness where the output can be described with integers.\nContinuous randomness is randomness where the output can be described with real numbers.\nRandomness is described by its patterns. Any individual data point may be random but collect enough of them from a given process/source and a pattern will emerge. Some values are more likely to occur, some are less likely, and some may even never occur.\n\n\n\n4.1.3 Properties of random variables\n\n4.1.3.1 Mean\nThe mean of a distribution is a measure of its “central tendencies”. Usually, values closer to the mean are more common than values further away from the mean. The mean of a set of random numbers can be calculated in R using the mean() function:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nmean(b)\n\n[1] 50.22\n\nmean(n)\n\n[1] 4.8852\n\n\nBecause of the randomness inherent in RNGs, the mean that you will get for any finite sample will often vary from sample to sample:\n\nb1 &lt;- rbinom(100, size = 100, prob = 0.5)\nb2 &lt;- rbinom(100, size = 100, prob = 0.5)\nb3 &lt;- rbinom(100, size = 100, prob = 0.5)\n\nmean(b1)\n\n[1] 50.6\n\nmean(b2)\n\n[1] 50.3\n\nmean(b3)\n\n[1] 50.42\n\n\nEvery distribution has some inherent mean given its set of parameters. You can think of this as the mean you would get if you took a really large sample. For example, the mean parameter in rnorm is exactly this large sample mean. You can see how the mean of a given sample tends to gets closer to this value as you increase the sample size:\n\nn10 &lt;- rnorm(10, mean = 5)\nn100 &lt;- rnorm(100, mean = 5)\nn1000 &lt;- rnorm(1000, mean = 5)\n\nmean(n10)\n\n[1] 5.112936\n\nmean(n100)\n\n[1] 5.047262\n\nmean(n1000)\n\n[1] 4.970296\n\n\nYou can calculate the mean yourself with some simple R code; it is the sum of all your data points divided by the number of data points:\n\nn100 &lt;- rnorm(100, mean = 5)\n\nmean(n100)\n\n[1] 4.868394\n\nsum(n100)/length(n100)\n\n[1] 4.868394\n\n\nFor any distribution, you can look up its mean (as well as the other properties discussed below) on sites like Wikipedia:\n\nBinomial distribution\nNormal distribution\n\n\n\n4.1.3.2 Variance/standard deviation\nA key feature of randomness is the amount of variability from outcome to outcome of a random process. As we will see when we start using models of data randomness to draw conclusions about said data, figuring out how variable our data is represents a key challenge.\nWe often describe the variability of a random sample using the variance and standard deviation. These can be calculated in R using the var and sd functions, respectively:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nsd(b)\n\n[1] 4.934429\n\nsd(n)\n\n[1] 0.9079115\n\nvar(b)\n\n[1] 24.34859\n\nvar(n)\n\n[1] 0.8243033\n\n\nClose inspection will reveal that the output of var is just the output of sd squared:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nsd(b)^2\n\n[1] 19.71717\n\nvar(b)\n\n[1] 19.71717\n\nsd(n)^2\n\n[1] 0.9586877\n\nvar(n)\n\n[1] 0.9586877\n\n\nThe variance measures the squared distance of each data point from the mean, and sums up all of these squared distances:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nvar(b)\n\n[1] 23.31828\n\nsum( ( b - mean(b) )^2 )/(length(b) - 1)\n\n[1] 23.31828\n\nvar(n)\n\n[1] 1.351635\n\nsum( ( n - mean(n) )^2 )/(length(n) - 1)\n\n[1] 1.351635\n\n\nWhy the square? When assessing variability, we don’t care if data is above or below the mean, so we want to score data points greater than the mean equally to how we score those below the mean. The square of the mean - 1 is the same as the square of the mean + 1, and both are positive values, because squaring a real number always yields a positive number. Why not the absolute value? Or the fourth power of the distance to the mean? Or any other definitively positive value? Convenience and mathematical formalism. In other words, don’t worry about it.\nNOTE: EXTRA DETAIL ALERT\nThe other mystery of the calculation of the standard deviation/variance is the fact that the sum of the deviations from the mean are divided by the number of data points minus 1, rather than just the number of data points, as in the mean. Why is this? Again, mathematical formalism, but there is an intuitive explanation to be discovered.\nWhen calculating the mean, each data point is its own entity that contributes equally to the calculation. I can’t calculate the mean without all n data points. When calculating the standard deviation though, each data point is compared to the mean, which is itself calculated from all of the data points.\nIf I tell you the mean though, I only need to tell you n - 1 data points for you to infer the final data point. For example, if I have 3 numbers that I tell you average to 2, and two of them are 1 and 3, what is the third? It has to be 2. Thus, when calculating the distance from each point to the mean, there are only n - 1 unique pieces of information. I tell you n - 1 data points and the mean, and you can tell me the standard deviation (since you can infer the missing data point). We say that there are “n - 1 degrees of freedom”. Thus, the effective average of the differences from the mean are all of the squared differences divided by n - 1, the number of “non-redundant” data points that remain after calculating the mean.\nEND OF THE EXTRA INFO ALERT\n\n\n4.1.3.3 Higher order “moments” (BONUS CONTENT)\nThe mean and the variance are related to what are called (who knows why) moments of a distribution. A moment is the average of some function of the random data. For example, the mean is the average of the data itself, which you could call data passed through the function f(x) = x. The variance is related to this moment, as well as the so-called second moment, which is the average of the square of the random data (f(x) = x^2). In fact, this leads to an equivalent way to calculate the variance:\n\nb &lt;- rbinom(100, size = 100, prob = 0.5)\nn &lt;- rnorm(100, mean = 5)\n\nvar(b)\n\n[1] 32.39758\n\nmean(b^2) - mean(b)^2\n\n[1] 32.0736\n\nvar(n)\n\n[1] 1.110824\n\nmean(n^2) - mean(n)^2\n\n[1] 1.099715\n\n\nWell not fully equivalent, but they get closer to one another as the sample size increases:\n\nn100 &lt;- rnorm(100, mean = 5)\nn1000 &lt;- rnorm(1000, mean = 5)\nn10000 &lt;- rnorm(10000, mean = 5)\n\n\n\nn = 100: \nvar(x):  1.023968 \n(x^2) - mean(x)^2:  1.013728 \n \nn = 1000: \nvar(x):  1.029771 \n(x^2) - mean(x)^2:  1.028741 \n \nn = 10000: \nvar(x):  1.014618 \nx^2) - mean(x)^2:  1.014517 \n \n\n\nVarious other moments or functions of moments enjoy some level of popularity in specific circles. We won’t talk about them here, but you’ll see them on Wikipedia sites for distributions, things like the “skewness” and “excess kurtosis” are examples of these additional moments, or functions of moments.\n\n\n4.1.3.4 Density/mass functions\nEarlier, I said “Think of [probability distributions] as functions, which take as input a number, and provide as output the probability of observing that number”. Fleshing out what I mean by this will help you understand one of the key concepts in probability theory: probability density/mass functions.\nTake a look at what a normalized histogram of draws from rbinom() looks like as we increase n. “Normalized” here means that instead of the y-axis representing the number of draws in a given bin (what is plotted by hist() by default), it is this divided by the total number of draws.\nb5 &lt;- rbinom(n = 10, size = 100, prob = 0.5)\nb50 &lt;- rbinom(n = 100, size = 100, prob = 0.5)\nb500 &lt;- rbinom(n = 1000, size = 100, prob = 0.5)\nb5000 &lt;- rbinom(n = 10000, size = 100, prob = 0.5)\n\n\n# freq = FALSE will plot normalized counts\nhist(b5, main = \"10 draws\", xlab = \"Random number\", freq = FALSE)\nhist(b50, main = \"100 draws\", xlab = \"Random number\", freq = FALSE)\nhist(b500, main = \"1000 draws\", xlab = \"Random number\", freq = FALSE)\nhist(b5000, main = \"10000 draws\", xlab = \"Random number\", freq = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you increase n, a pattern begins to emerge. The y-axis is the fraction of the time a random number ended up in a particular bin. It can thus be interpreted as the probability that the random number falls within the bounds defined by that bin. What you may notice is that these probabilities seem to converge to particular values as n gets large. This pattern of probabilities is known as the binomial distributions “probability mass function”. If the output of the RNG is continuous real numbers, then this is referred to as the “probability density function”.\nFor all of the distributions that we can draw random numbers from in R, we can similarly use R to figure out its probability mass/density function. The function will have the form d&lt;distribution&gt;, where d denotes density function:\n\ndbinom(x = 50, size = 100, prob = 0.5)\n\n[1] 0.07958924\n\ndbinom(x = 10, size = 100, prob = 0.5)\n\n[1] 1.365543e-17\n\ndbinom(x = 90, size = 100, prob = 0.5)\n\n[1] 1.365543e-17\n\ndbinom(x = 110, size = 100, prob = 0.5)\n\n[1] 0\n\ndbinom(x = 50.5, size = 100, prob = 0.5)\n\nWarning in dbinom(x = 50.5, size = 100, prob = 0.5): non-integer x = 50.500000\n\n\n[1] 0\n\n\n\n4.1.3.4.1 Exercise\nProve to yourself that dbinom yield probabilities similar to what you saw in your normalized histograms.\nHint:\n\n### Make bins of size 1 so as to match up more cleanly with `dbinom()`\n\n# Maximum value\nsize &lt;- 100\n\n# Plot with bin size of 1\nhist(rbinom(1000, size, 0.5), freq = FALSE, breaks = 1:size)\n\n\n\n\n\n\n\n\nEXAMPLE EXERCISE:\nlibrary(ggplot2)\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Histogram\nrns &lt;- rbinom(10000, 100, 0.5)\n\n# dbinom\nvalues &lt;- 0:100\nprobs &lt;- dbinom(values, size = 100, prob = 0.5)\n\ntibble(x = rns) %&gt;%\n  ggplot(aes(x = x)) +\n  theme_classic() + \n  geom_histogram(binwidth = 1, aes(y=..count../sum(..count..))) +\n  xlab(\"Value\") +\n  ylab(\"Proportion\") +\n  xlim(c(0, 100)) + \n  ylim(c(0, max(probs + 0.01)))\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\ntibble(x = values,\n       y = probs) %&gt;%\n  ggplot(aes(x = x, y = y)) + \n  geom_bar(stat = \"identity\") + \n  theme_classic() +\n  xlab(\"Value\") +\n  ylab(\"Probability\") +\n  xlim(c(0, 100)) + \n  ylim(c(0, max(probs + 0.01)))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\n\n\n\nEND OF EXAMPLE EXERCISE\nIn the case of discrete random variables, this exercise shows that the output of d&lt;distribution&gt;() has a clear interpretation: its the probability of seeing the specified value given the specified distribution parameters. In the case of continuous random variables though, the interpretation is a bit trickier. For example, consider an example output of dnorm():\n\ndnorm(0, mean = 0, sd = 0.25)\n\n[1] 1.595769\n\n\nIt’s greater than 1! How could a probability be greater than 1?? The answer is that the output isn’t a probability in the same way as in the discrete case. All that matters for this class is that this number is proportional to a probability. The distinction here is a bit unintuitive, but there are some great resources for fleshing out what this means. See for example 3blue1brown’s video on the topic.\nBEGINNING OF BONUS CONTENT\nHere I will briefly explain what the output of dnorm() represents.\nThe output of dnorm() represents a “probability density”, hence the name “probability density function”. What is a “probability density”? The analogy to an object’s mass and density is fitting. If you want to get something’s mass given its density, you need to multiply its density by that object’s volume. To get probability (mass) from a density (output of dnorm()) you need to specify how large of a bin to consider.\n\ndnorm(0, mean = 0, sd = 0.25)*0.001\n\n[1] 0.001595769\n\n\nThis can be interpreted as the probability of a number generated by rnorm() falling between -0.0005 (i.e., 0 - 0.001/2) and 0.0005. This is only an approximation, as the density (output of dnorm()) is not constant between -0.0005 and 0.0005:\n\ndnorm(-0.0005, mean = 0, sd = 0.25)\n\n[1] 1.595766\n\ndnorm(0, mean = 0, sd = 0.25)\n\n[1] 1.595769\n\n\nChoose a small enough bin though, and it will be pretty close to the probability of a number ending up in that bin. The exact answer to this question comes from integrating the probability density function within the bin of interest:\n\\[\n\\text{P(x} \\in [L, U]\\text{)} = \\int_{L}^{U} \\text{dnorm(x, mean = 0, sd = 0.25) dx}  \n\\]\n\\(\\text{P(x} \\in [L, U]\\text{)}\\) should be read as “the probability that a random variable x drawn from rnorm() falls between \\(L\\) and \\(U\\)”.\n!!END OF LECTURE 1!!",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#discrete-random-variables",
    "href": "probability.html#discrete-random-variables",
    "title": "3  Introduction to Probability",
    "section": "4.2 Discrete random variables",
    "text": "4.2 Discrete random variables\n\n4.2.1 Modeling two outcomes: the binomial distribution\nYou have already played around with the binomial distribution, as that is the name given to the distribution of numbers generated by rbinom. Now it is time to tell its story. All distributions have a story, or rather, a generative model. A generative model is a process that would give rise to data following a particular distribution.\nThe binomial’s story goes like this: imagine you have an event that can result in two possible outcomes. For example, flipping a coin (an event) can result in either a heads or a tails (two possible outcomes). One thing has to be true about this process for it to be well described by a binomial distribution: the probability of a particular outcome must be exactly the same from event to event. For example, if every time you flip a coin, it has a 50% chance of coming up heads and a 50% chance of coming up tails, then the number of heads is well described by a binomial distribution.\nThe size parameter in the rbinom function sets the number of events you want to simulate. The prob parameter sets the probability of an outcome termed the “success”. Success here has a misleading connotation; it might represent an outcome you are happy about, it might represent an outcome that you displeased by, it might represent an outcome that you are completely indifferent to. Statisticians name things in funny ways…\n\n4.2.1.1 Exercise\nTake some time to explore the properties and output of rbinom(). Questions to consider include:\n\nHow does the mean depend on size?\nHow does the mean depend on prob?\nHow does the variance depend on size? prob?\nWhat is the most likely outcome for a given size and prob?\nWhat is an aspect of RNA-seq data that you could model with a binomial distribution?\n\n\n\n4.2.1.2 Example exercise:\nGoing hardcore: inspect properties of distribution as function of size and prob\nlibrary(dplyr)\nlibrary(ggplot2)\n\nnsims &lt;- 5000\n\nprobs &lt;- seq(from = 0, to = 1, by = 0.1)\nsizes &lt;-  seq(from = 0, to = 100, by = 10)\n\nLp &lt;- length(probs)\nLs &lt;- length(sizes)\n\n\nmeans &lt;- rep(0, times = Lp*Ls)\nvars &lt;- means\n\ncount &lt;- 1\nfor(p in seq_along(probs)){\n  \n  for(s in seq_along(sizes)){\n    \n    simdata &lt;- rbinom(nsims, size = sizes[s], prob = probs[p])\n    \n    means[count] &lt;- mean(simdata)\n    vars[count] &lt;- var(simdata)\n    count &lt;- count + 1\n    \n  }\n  \n}\n\n\nsim_df &lt;- dplyr::tibble(prob = rep(probs, each = Ls),\n                        size = rep(sizes, times = Lp),\n                        mean = means,\n                        variance = vars) \n\nsim_df %&gt;%\n  ggplot(aes(x = size, y = prob, fill = mean)) + \n  geom_tile() + \n  scale_fill_viridis_c() + \n  theme_classic() + \n  xlab(\"Size\") + \n  ylab(\"Prob\")\nsim_df %&gt;%\n  ggplot(aes(x = size, y = prob, fill = variance)) + \n  geom_tile() + \n  scale_fill_viridis_c() + \n  theme_classic() + \n  xlab(\"Size\") + \n  ylab(\"Prob\")\n\n\n\n\n\n\n\n\n\n\nObservations:\n\nConfirms that higher size and prob = higher average number\nInteresting to see that variance increases as a function of size, but that prob = 0.5 leads to the highest variance.\nSetting prob to 0 or 1 causes variance to go exactly 0, regardless of what size is set to. Similarly, setting prob to 0 causes the mean to go to exactly 0, regardless of what size is set to.\n\n\n\n\n4.2.2 Modeling &gt; 2 outcomes: the multinomial distribution\nWhen exploring the binomial distribution, we considered data with two possible outcomes. What if many of the same assumptions hold, but there are now more than 2 possible results? That is where the multinomial distribution comes in.\nGenerative model: Imagine you have an event that can result in one of N outcomes, where N is some integer. Each outcome has some probability, p_{i} (i denoting the ith outcome, for i between 1 and N, inclusive) of occurring, and all of the p_{i} remain constant from event to event.\nExample case: When rolling a die, there are 6 outcomes of what number shows face up when the die stops rolling (1, 2, 3, 4, 5, or 6). If each face is equally likely to show up with each roll, then p_{i} = 1/6 for all i. This can be simulated in R with rmultinom():\n\nrmultinom(5, size = 1, prob = rep(1/6, times = 6))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    0    0    0    0    1\n[3,]    0    0    0    0    0\n[4,]    1    0    1    1    0\n[5,]    0    1    0    0    0\n[6,]    0    0    0    0    0\n\n\nThe output will be a matrix with n columns and number of rows equal to the length of prob. The value in the ith row and jth column is the number of times event i was observed in simulation j. In this case, the rows can be interpreted as the number a die came up, and each columns represents the result of a single role of a single die.\n\n4.2.2.1 Exercise\nUse rmultinom() and a bit of ancillary code to simulate the sequence of an RNA produced from a gene. This doesn’t need to be a real gene, just a random sequence of nucleotides. Then determine some features of this sequence:\n\nWhat is the longest run of each nucleotide in a row?\nWhat is the least common nucleotide in your sequence?\nWhat is the most common nucleotide in your sequence?\nHow different is the rate of occurence of the least and most common nucleotides\n\n\n\n4.2.2.2 Example exercise\n\n### Parameters for simulation\n\n# Number of nucleotides in RNA\nlength &lt;- 25\n\n# Nucleotide proportions\nATprop &lt;- 0.4\nCGprop &lt;- 0.6\n\n### Generate random nucleotides\nnucleotides &lt;- rmultinom(length, size = 1,\n                         prob = c(ATprop/2, ATprop/2,\n                                  CGprop/2, CGprop/2))\n\n### Determine sequence\nnucleotide_types &lt;- c(\"A\", \"U\", \"C\", \"G\")\nnucleotide_ids &lt;- 1:4\n\n# Cute matrix algebra trick\n  # IDs will be row ID for which value of matrix was 1\nIDs &lt;- t(nucleotides) %*% matrix(1:4, nrow = 4, ncol = 1)\nsequence &lt;- paste(nucleotide_types[IDs], collapse = \"\")\n\n\nprint(sequence)\n\n[1] \"GCAUCCCAACUGCUUUUCAGCUACG\"\n\n\n\n\n\n4.2.3 Modeling “counts”: the Poisson distribution\nAs we have seen, RNA-seq data can be summarized as a matrix of read counts. Read counts are the number of times that we observed an RNA fragment from a particular genomic feature. What then, is a good model for the number of read counts for a particular feature? Enter a major contender: the Poisson distribution\nGenerative model: Imagine that “events” are independent, that is the time since the last event or how many such events have already occurred have no bearing on how likely we are to see another event. The number of such events in a particular period of “time” (time here in quotes as time could represent units like “number of RNA fragments sequenced”) will be Poisson distributed.\n\n4.2.3.1 Exercise\n\nExplore the output of rpois(), the Poisson distribution random number generator. Besides the typical n parameter, it has only one other parameter, lambda. Investigate the impact of changing lambda.\nCompare the amount of variability in rpois() output to the amount of variability in a real RNA-seq dataset.\n\n\n\n\n4.2.4 Modeling time to first “success”: the geometric distribution\nThe binomial distribution provides a great model for the number of “successes” in a set of random, binary outcomes. What if we were interested in how long we will have to wait until the first success though? That is where the geometric distribution comes into play.\nGenerative Model: Like with the binomial distribution, imagine you have an event that can result in two possible outcomes, and the probability of a particular outcome is exactly the same from event to event. The number of events until you see until the first success will follow a geometric distribution.\n\n4.2.4.1 Exercise\n\nExplore the output of rgeom(), the geometric distribution random number generator. Besides the typical n, it has only one additional parameter, prob, which can be a number between 0 and 1.\nUse rbinom() and a bit of custom R code to create your own rgeom() alternative, and confirm that the average value given by your simulator and rgeom() for a given prob is approximately equal.\n\n\n\n\n4.2.5 Modeling time to nth “success”: the negative binomial distribution\nThe geometric distribution describes the time until the first success of a binary outcome. What if we need to model the time to the nth success, where n is any integer &gt; 1? For that, we can look to the negative binomial distribution.\nGenerative Model: Like with the binomial distribution, imagine you have an event that can result in two possible outcomes, and the probability of a particular outcome is exactly the same from event to event. The number of events until you see until the nth success will follow a negative binomial distribution.\nSpoiler alert: We’ll see the negative binomial later, but it will take on a very different character, acting as a generalization of the Poisson distribution rather than of the geometric distribution. This just goes to show that these distributions can wear many hats and will often have many distinct back stories.\n\n4.2.5.1 Exercises\n\nExplore the output of rnbinom(). In addition to the standard n, you should play around with the two rnbinom() unique parameters relevant to the generative model laid out above: size (number of successes) and prob (probability of a success).\nCreate your own rnbinom() simulator using rbinom() and some custom R code. Compare the properties of your simulator and rnbinom() to ensure things are working as expected.\n\n\n\n\n4.2.6 Sampling without replacement: the hypergeometric distribution\nThe last major discrete distribution to discuss is the hypergeometric distribution. Every single distribution we have looked at thus far has an assumption of event-to-event independence. That is, no matter what has happened previously, the probability of future events is unwaivering. Can you imagine any case where this might not be true?\nThe simplest way to violate this assumption is to consider the process of “sampling without replacement”. Say you have a bag of marbles of two colors, black and red. Now imagine that you draw one from the bag, assess its color. What distribution describes the number of black marbles you see? If each time you draw a marble, you subsequently return it to the bag, then the number of black marbles would be well modeled as binomially distributed. This would be called “sampling with replacement”, and it yields independence between each draw. The contents of the bag never changes so neither do the probabilities of a particular result. What if you DIDN’T return each marble though? This would be called “sampling without replacement”, and now the result of the last draw matters, because it affects how likely each outcome is in the next draw. Describing the number of black marbles in this case requires a new distribution: the hypergeometric distribution.\n\n4.2.6.1 Exercise\n\nExplore the output of rhyper. Its parameters are nn (what is referred to as n in all of the obther r&lt;dist&gt;() functions), m (think of this as the number of red marbles), n (think of this as the number of black marbles), and k (think of this as the number of marbles drawn from the bag).\n\n!!END OF LECTURE 2!!",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#continuous-random-variables",
    "href": "probability.html#continuous-random-variables",
    "title": "3  Introduction to Probability",
    "section": "4.3 Continuous random variables",
    "text": "4.3 Continuous random variables\nUp until now, we have focused on describing “discrete randomness”. This means that outcomes of the processes that we considered had to be well described with integers (0, 1, 2, …). Not everything in the world fits this description though. Consider the following types of data/processes we could imagine modeling:\n\nThe heights of college-aged students\nThe probability of contracting COVID\n\n\nIn these cases, the outcome is best described using a real number, that is a number which can have an arbitrary number of decimal places. We refer to these as “continuous random variables”, and in this lecture we will familiarize ourselves with the most popular distributions for modeling such processes.\n\n4.3.1 Pure randomness: the uniform distribution\nWhat’s the first thing that comes to mind when you hear that something is “random”? At this point, you should be conditioned to start thinking about the underlying generative model and the distribution that could describe said randomness, but what if you had been asked about randomness embarking on this class? I would argue that for most people, the default definition of randomness is “pure randomness”: every possible event is equally likely. While this class should ensure this is no longer your default, there are plenty of processes that are well described by this sort of randomness. For those cases, we have the uniform distribution.\nGenerative model: Imagine the output of a process can be described as a real number between a and b. If all values between a and b are equally likely, then this process’ output will be well modeled with a uniform distribution\n\n4.3.1.1 Exercise\n\nExplore the output of runif(). It has 3 parameters of note. The first is the same for all RNGs and is called n. It represents the number of random numbers it spits out. The other two are called min and max. Both of these can be any number you want, as long as min &lt;= max. Generate some random numbers with runif and observe there properties.\n\n\n\n\n4.3.2 Generalizing the uniform distribution\n\n4.3.2.1 Exercises\n\nExplore the output of rbeta(). Check out ?rbeta() to see what parameters exist.\nCombine rbeta() and rbinom(), using the former to simulate values of p for the latter. Compare this to rbinom() with prob = shape1 / (shape1 + shape2). How are they similar? How do they differ?\n\n\n\n\n4.3.3 Modeling the time until an event: the exponential distribution\n\n4.3.3.1 Exercises\n\nExplore the output of rexp(). Check out ?rexp() to see what parameters exist.\nCompare the following distributions:\n\n\nThe full output of a run of rexp() with a large n, like 100,000\nThat same output, but subtracting the first quartile from all of the values, and throwing out any negative values. You can find what the first quartile is with quartile().\n\n\nDo the same thing as in 2 but with the output of rbeta(). Do you get the same result as in 2?\nDo the same thing as in 2 and 3 but with the output of rgeom(). Do you get the same result as in 2 or as in 3?\n\nExercises 2-4 explore the property known as memorylessness, which is only possessed by two distributions in the entire universe of distributions. This property in some sense defines these two distributions.\n\n\n\n4.3.4 Generalizing the exponential distribution: the gamma distribution\n\n4.3.4.1 Exercises\n\nCompare the output of rgamma() with shape = 1 to that of rexp(). Make some plots to convince yourself that these are the same.\nExplore how changing the shape parameter affects how the output of rgamma() differs from that of rexp().\n\n\n\n\n4.3.5 All distributions lead here: the normal distribution\n\n4.3.5.1 Exercises\n\nChoose your favorite continuous distribution to this point. Use its associated RNG to follow these steps:\n\n\n\nGenerate N samples from your distribution of choice with whatever parameters you desire.\n\n\nCalculate the average of those N samples with mean()\n\n\nRepeat a) and b) M times and save the result of b) each time. You should use a for loop for this.\n\n\nPlot a histogram of the sample means.\n\n\nCompare your plot to a histogram from rnorm(M, mean = E, sd = S), where E = mean(means) and S = sd(means).\n\n\nExplore the impact of varying N, M, and the parameters of your distribution of choice on the plots generated in e).\n\n\n!!END OF LECTURE 3!!",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#problem-set-simulating-data-with-distributions",
    "href": "probability.html#problem-set-simulating-data-with-distributions",
    "title": "3  Introduction to Probability",
    "section": "4.4 Problem set: Simulating data with distributions",
    "text": "4.4 Problem set: Simulating data with distributions\n\n4.4.1 RNA-seq data\nTry and simulate a count matrix that has similar properties to that of a real RNA-seq count matrix. This is a fairly open-ended exercise and is meant as an exercise in thinking carefully about modeling data with probability distributions. Compare your simulated data to the real thing, making some plots to assess their similarity.\n\n\n4.4.2 Poisson Process\nThis exercise will teach you how to use what you have learned to simulate what is known as a Poisson process. The strategy employed here is known as Gillespie’s algorithm, and is widely used in the computational modeling of biochemical reactions.\nImplement a simulation of RNA transcription following these steps:\n\nSpecify two parameters at the top of your code:\n\n\nT: the length of time for which to simulate.\nrate: The rate at which RNA molecules are synthesized.\n\n\nSet a couple variables that will change throughout the simulation:\n\n\ncurrent_t: the current time in the simulation. Set to 0.\nRNA_cnt: the number of RNAs that have been produced. Set to 0.\n\n\nIn a while loop, add the output of rexp(n = 1, rate = rate) to current_t. If the new value of current_t is &gt; T, break out of the while loop. If not, then add one to RNA_cnt.\n\nExplore some properties of your simulation:\n\nMake a plot of the number of RNAs as a function of time. Make this plot with three different values of rate.\nRun the simulation multiple times with a given T and rate, and compare the distribution of RNA_cnt’s to that of a Poisson distribution with lambda = rate. Are they the same? You are discovering why this is called a Poisson process.\n\nEXTRA CREDIT\nSimulate RNA synthesis and degradation, making a plot of the amount of undegraded RNA as a function of time. Some things you will need to know:\n\nIf you have multiple processes whose time until the next event is exponentially distributed, the time until any of these events occurs is also exponentially distributed, with rate = sum of the rates of all of the individual exponential distributions.\nIf you have multiple processes whose time until the next event is exponentially distributed, the probability that event i is the next event = \\(rate_{i}/\\sum rate_{j}\\), where \\(rate_{i}\\) represents the rate parameter for the ith events exponential distribution, and the sum is over the rates of all of the processes’ associated exponential distributions.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#appendix-probability-distributions",
    "href": "probability.html#appendix-probability-distributions",
    "title": "3  Introduction to Probability",
    "section": "Appendix: Probability Distributions",
    "text": "Appendix: Probability Distributions\nWhen I claim that “there is randomness in your data”, what does that mean? For some people, the term “randomness” implies complete unpredictability. Such people would interpret my claim to mean that every time you collect a new replicate, any value for the thing you are measuring is fair game and equally likely. “You got 100 reads from the MYC gene in your last RNA-seq dataset? Well don’t be surprised if you get 1000, or 10000, or 0 reads next time!” You may call this uniform randomness. You can generate such data right here in R, using the runif() function:\n\ndata &lt;- runif(n = 10, min = 0, max = 100)\n\nhist(data)\n\n\n\n\n\n\n\n# Check out the individual data points\nprint(data)\n\n [1] 53.545984 90.292422 92.639386 87.534476 90.411141 57.240365 85.303398\n [8]  9.770569 48.690104 41.881876\n\n\nrunif() has three parameters: 1) n specifies the number of numbers to generate, 2) min specifies the minimum number it could possibly generate, and 3) max specifies the maximum number it could possibly generate. In the above example, I am thus creating 10 numbers between 0 and 100, with every number in between being equally likely to pop out. Generate a lot more numbers and this uniform pattern of appearance becomes much more clear:\n\ndata &lt;- runif(n = 1000, min = 0, max = 100)\n\nhist(data)\n\n\n\n\n\n\n\n\nWhile this definition of randomness is intuitive, it can’t be the only type of randomness. If RNA-seq data were this random, it would be useless! There is nothing to learn from measurements that can take on any value with equal probability.\nThus, to describe all of the kinds of randomness we see in the real world, it is important to expand our definition beyond uniform randomness. Enter the probability distribution. A probability distribution is like a function in R. It takes as input a number (or maybe a set of numbers), and provides as output, the probability of seeing that number. For uniformly random data this might look something like:\n\nuniform_distribution &lt;- function(data, min = 0, max = 100){\n  \n  if(data &gt;= min & data &lt;= max){\n    \n    output &lt;- 1\n    \n  }else{\n    \n    output &lt;- 0\n    \n  }\n  \n  return(output)\n  \n}\n\nThat is to say, as long as the data is within the bounds of what is possible, it has the same probability of occurring; you get the same number out from this function. This function would make mathematicians",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "4  Introduction to Statistical Modeling",
    "section": "",
    "text": "4.1 Developing an intuition for model fitting\nThis exercise will walk you through how to think about what it means to “fit” a model. The key tool in our arsenal is going to be simulation, where we create data that follows known distributions. Your task, if you are willing to accep it, is to figure out how to recover the parameters you used in your simulation from the noisy data generated by the simulator. Are you ready? I’ll start with a guided exercise, and then present some open-ended exercises to get you exploring model fitting.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Statistical Modeling</span>"
    ]
  },
  {
    "objectID": "modeling.html#developing-an-intuition-for-model-fitting",
    "href": "modeling.html#developing-an-intuition-for-model-fitting",
    "title": "4  Introduction to Statistical Modeling",
    "section": "",
    "text": "4.1.1 Guided exercise\nStep 1: Simulate some data\nFor this exercise, I am going to simulate binomially distributed data, and devise a strategy to figure out what prob I used in this simulation. Obviously, I will know what prob I used, but in the real world, you won’t have access to the true parameters of the universe. Knowing the truth helps us know if we are on the right track though, and is why simulations are such a useful tool:\n\n# Set the seed so you can reproduce my results\nset.seed(42)\nsimdata &lt;- rbinom(100, 100, 0.5)\n\nTechnically, there are two parameters that I had to set here, size and prob. In this case, I am going to assume that size is data I have access to. Usually, if we are modeling something as binomially distributed, we will know how many trials there were.\nStep 2: Visualize your data\nLet’s see what the data looks like:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsim_df &lt;- tibble(successes = simdata)\n\nsim_plot &lt;- sim_df %&gt;%\n  ggplot(aes(x = successes)) + \n  geom_histogram() +\n  theme_classic()\nsim_plot\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nDoes the data look at all surprising? We set prob equal to 0.5, so on average, we expect 50% of the trials to end in successes. If we perfectly hit this mark in our finite sample, that would mean 50 successes. Let’s annotate this mark on the plot\n\nsim_plot +\n  geom_vline(xintercept = 50,\n             color = 'darkred')\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIt’s a bit noisy, but the data does seem to be roughly centered around 50. That’s a good sign that our simulation worked, but now we need to think about how we would have figured out that the prob in this case was 0.5\nStep 3: Fight simulations with simulations\nOur goal is to find a binomial distribution prob parameter that accurately describes this data. An intuitive way to think about doing this is to simulate data with candidate values for prob, and see how similar the simulated data is to the real data. Here’s how you might do that:\n\n### With a for loop\nlibrary(tidyr)\ncandidates &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9)\n\nsim_list &lt;- vector(mode = \"list\",\n                  length = length(candidates))\nfor(c in seq_along(candidates)){\n  \n  sim_list[[c]] &lt;- rbinom(100, 100, candidates[c])\n  \n}\nnames(sim_list) &lt;- as.factor(candidates)\nsim_df &lt;- as_tibble(sim_list) %&gt;%\n  pivot_longer(names_to = \"prob\",\n               values_to = \"successes\",\n               cols = everything())\n\nsim_df %&gt;%\n  ggplot(aes(x = successes, \n             fill = prob, \n             color = prob)) + \n  geom_histogram(alpha = 0.1,\n                 position = 'identity') +\n  geom_histogram(data = tibble(successes = simdata,\n                               prob = factor('data')),\n                 aes(x = successes),\n                 color = 'black',\n                 alpha = 0.1,\n                 fill = 'darkgray') +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() + \n  theme_classic() +\n  ggtitle('data in black/gray') + \n  xlim(c(0, 100))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 10 rows containing missing values (`geom_bar()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\nsim_df %&gt;%\n  ggplot(aes(x = successes, \n             fill = prob, \n             color = prob)) + \n  geom_density(alpha = 0.1,\n                 position = 'identity') +\n  geom_density(data = tibble(successes = simdata,\n                               prob = factor('data')),\n                 aes(x = successes),\n                 color = 'black',\n                 alpha = 0.1,\n                 fill = 'darkgray') +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() + \n  theme_classic() +\n  ggtitle('data in black/gray') + \n  xlim(c(0, 100))\n\n\n\n\n\n\n\n\nOf these small subset of candidate prob values, 0.5 is the clear winner. The overlap of the simulation with the data is striking, and strongly suggests that this is a good fit to our data.\nOf course, we know the true value is 0.5, and this knowledge guided our choice of candidates. So let’s explore a different range of candidates, all of which are much closer to the known truth:\n\n### With a for loop\nlibrary(tidyr)\ncandidates &lt;- c(0.48, 0.49, 0.5, 0.51, 0.52)\n\nsim_list &lt;- vector(mode = \"list\",\n                  length = length(candidates))\nfor(c in seq_along(candidates)){\n  \n  sim_list[[c]] &lt;- rbinom(100, 100, candidates[c])\n  \n}\nnames(sim_list) &lt;- as.factor(candidates)\nsim_df &lt;- as_tibble(sim_list) %&gt;%\n  pivot_longer(names_to = \"prob\",\n               values_to = \"successes\",\n               cols = everything())\n\n\nsim_df %&gt;%\n  ggplot(aes(x = successes, \n             fill = prob, \n             color = prob)) + \n  geom_histogram(alpha = 0.1,\n                 position = 'identity') +\n  geom_histogram(data = tibble(successes = simdata,\n                               prob = factor('data')),\n                 aes(x = successes),\n                 color = 'black',\n                 alpha = 0.1,\n                 fill = 'darkgray') +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() + \n  theme_classic() +\n  ggtitle('data in black/gray') + \n  xlim(c(0, 100))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 10 rows containing missing values (`geom_bar()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n\nsim_df %&gt;%\n  ggplot(aes(x = successes, \n             fill = prob, \n             color = prob)) + \n  geom_density(alpha = 0.1,\n                 position = 'identity') +\n  geom_density(data = tibble(successes = simdata,\n                               prob = factor('data')),\n                 aes(x = successes),\n                 color = 'black',\n                 alpha = 0.1,\n                 fill = 'darkgray') +\n  scale_fill_viridis_d() +\n  scale_color_viridis_d() + \n  theme_classic() +\n  ggtitle('data in black/gray') + \n  xlim(c(0, 100))\n\n\n\n\n\n\n\n\nNow the comparisons are much messier. Sure, 0.5 is a great match, but none of these chosen values yield simulations too different from the truth.\nFrom this, it is clear that there are ranges of values that we can confidently rule at as good conclusions for what prob best describes our data. Values of 0.25 or less, and values of 0.75 or more are very bad fits. There is also good evidence that a good fit is somewhere around 0.5, but the exact value that would represent the best guess is uncertain.\nStep 4: Make fitting more rigorous\nThe current strategy we have employed could be referred to as “simulations and vibes”. We have simulated data with some values for the unknown parameter in question, and assessed by eye how cloes our simulated data matched our real data. Whlie this has offered some valuable insights, it’s important to recognize the fundamental limitations of such an approach:\n\nEach simulation with a given parameter value will yield different results. Random number generators are going to be random. One output of prob = 0.5 might look exactly like the real think, but so might one value of prob = 0.48.\nIt’s time intensive. You have to simulate data for a bunch of different values, and then painstakingly stare at cluttered plots trying to make sense of which simulation fit your data the best.\nIt’s subjective. None of our simulations matched the data exactly, and even if one run of a particular simulation did, see point 1 for why that can’t be considered conclusive evidence in favor of that parameter being the best.\n\nHow can we fix this problem? What we need is a quantitative metric, a number that we can assign to every possible value of prob that describes how good of a fit it is to our data. It should have the following properties:\n\nIt should be deterministic; a given value of prob should yield a single unique value for this metric.\nHigher values should represent better fits.\nThe more data we have, the better this metric should do at predicting the true parameter value.\n\nSit and think about this for a while (maybe a couple decades, which is what it took the field to converge on this solution), and you’ll arrive at something called the “total likelihood”. In this part of the exercise, we’ll develop an intuition for this concept:\n\n4.1.1.1 What is the likelihood?\nWhen discussing a particular distribution, we have playing with its associated random number generator function in R. For the binomial distribution, this is the rbinom() function. If you check the documentation for rbinom() though, with ?rbinom, you’ll see that you actually get documentation for 4 different functions, all with a similar naming convention! We’ll find use for all of these at some point in this class, but for now, let’s focus on one that we briefly visited last week, dbinom().\nUnlike rbinom(), dbinom() returns the same value for a given set of parameters every single time. It is not random:\n\ndbinom(50, 100, 0.5)\ndbinom(50, 100, 0.5)\n\n[1] 0.07958924\n[1] 0.07958924\n\n\nWhat does this value represent though. First, let’s understand its input:\n\nx: The number of “successes”. So essentially what you could get out from a given run of rbinom().\nsize: Same as in rbinom(), the number of trials.\nprob: Same as in rbinom(), the probability that each trial is a success.\n\nSo it seems to quantify something about a particular possible outcome of rbinom(). It assigns some number to an outcome given the parameters you could have used to simulate such an outcome. What does this number represent though? To find out, let’s plot it for a range of x’s:\n\nxs &lt;- 0:100\nds &lt;- dbinom(xs, 100, 0.5)\n\ntibble(x = xs,\n       d = ds) %&gt;%\n  ggplot(aes(x = x, y = d)) + \n  geom_point() + \n  theme_classic()\n\n\n\n\n\n\n\n\nValues closer to 50 get higher numbers than values further from 50, and the plot seems to be symmetric around 50. What’s significant about 50? It’s \\[size * prob\\], or the average value we would expect to get from rbinom() with these parameters!\nThis investigation should give you some sense that the output of dbinom() is in some way related to the probability of seeing x given a certain size and prob. Is it exactly this probability? We can gut check by assessing some cases we know for certain. Like, what is the probability of seeing 1 success in 1 trial if prob = 0.5? 0.5, because that’s the defintiion of prob! What does dbinom() give us in this scenario:\n\ndbinom(1, 1, 0.5)\n\n[1] 0.5\n\n\n0.5; that’s a good sign that we are on to something. Try out some different values of prob:\n\ndbinom(1, 1, 0.3)\n\n[1] 0.3\n\ndbinom(1, 1, 0.7)\n\n[1] 0.7\n\ndbinom(1, 1, 0.2315)\n\n[1] 0.2315\n\n\nEverything still checks out. How about the probability of 2 successes in 2 trials given a certain prob? Each trial has probability of prob of being a success, and each trial is independent. Therefore, the probability of 2 successes in 2 trials is \\[prob * prob\\]. Does dbinom() give us the same output in that case?\n\n# Expecting 0.25\ndbinom(2, 2, 0.5)\n\n[1] 0.25\n\n# Expecting 0.01\ndbinom(2, 2, 0.1)\n\n[1] 0.01\n\n\nSure thing!\nConclusion: dbinom(x, size, prob), tells us the probability of seeing x successes in size trials given the probability of a success is prob.",
    "crumbs": [
      "Necessary Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Statistical Modeling</span>"
    ]
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "lm.html",
    "href": "lm.html",
    "title": "6  Linear Modeling",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Linear Modeling</span>"
    ]
  },
  {
    "objectID": "dreduce.html",
    "href": "dreduce.html",
    "title": "7  Dimensionality Reduction",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dimensionality Reduction</span>"
    ]
  },
  {
    "objectID": "cluster.html",
    "href": "cluster.html",
    "title": "8  Clustering and Mixture Modeling",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Popular Methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clustering and Mixture Modeling</span>"
    ]
  },
  {
    "objectID": "bioinfo.html",
    "href": "bioinfo.html",
    "title": "9  Practical Bioinformatics",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Statistics in the Wild",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Practical Bioinformatics</span>"
    ]
  },
  {
    "objectID": "finalproject.html",
    "href": "finalproject.html",
    "title": "10  Analysis of an RNA-seq dataset",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Statistics in the Wild",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analysis of an RNA-seq dataset</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]