# Introduction to Statistical Modeling

The challenge we are faced with when analyzing RNA-seq
data, or any data for that matter, is to draw conclusions in a way that accounts for the randomness inherit in our data. In the last chapter, we introduced a number of tools, i.e., probability distributions, for describing the common types of randomness. Once we have a sense of how to describe the variance in our data, i.e., the combination of probability distributions that represent good "models" of our data, how do we put these models to use? The answer to this question is known as "fitting the model to your data", and is the topic of this chapter.

## Developing an intuition for model fitting

This exercise will walk you through how to think about what it means to "fit" a model. The key tool in our arsenal is going to be simulation, where we create data that follows known distributions. Your task, if you are willing to accept it, is to figure out how to recover the parameters you used in your simulation from the noisy data generated by the simulator. Are you ready? I'll start with a guided exercise, and then present some open-ended exercises to get you exploring model fitting.

### Guided exercise

**Step 1: Simulate some data**

For this exercise, I am going to simulate binomially distributed data, and devise a strategy to figure out what `prob` I used in this simulation. Obviously, I will know what `prob` I used, but in the real world, you won't have access to the true parameters of the universe. Knowing the truth helps us know if we are on the right track though, and is why simulations are such a useful tool:

```{r}
# Set the seed so you can reproduce my results
set.seed(42)
simdata <- rbinom(100, 100, 0.5)
```

Technically, there are two parameters that I had to
set here, `size` and `prob`. In this case, I am going to assume that `size` is data I have access to. Usually, if we are modeling something as binomially distributed, we will know how many trials there were. 

**Step 2: Visualize your data**

Let's see what the data looks like:

```{r}
library(ggplot2)
library(dplyr)

sim_df <- tibble(successes = simdata)

sim_plot <- sim_df %>%
  ggplot(aes(x = successes)) + 
  geom_histogram() +
  theme_classic()
sim_plot
```

Does the data look at all surprising? We set `prob` equal to 0.5, so on average, we expect 50% of the trials to end in successes. If we perfectly hit this mark in our finite sample, that would mean 50 successes. Let's annotate this mark on the plot

```{r}
sim_plot +
  geom_vline(xintercept = 50,
             color = 'darkred')
```

It's a bit noisy, but the data does seem to be roughly centered around 50. That's a good sign that our simulation worked, but now we need to think about how we would have figured out that the `prob` in this case was 0.5

**Step 3: Fight simulations with simulations**

Our goal is to find a binomial distribution `prob` parameter that accurately describes this data. An intuitive way to think about doing this is to simulate data with candidate values for `prob`, and see how similar the simulated data is to the real data. Here's how you might do that:

```{r}
### With a for loop
library(tidyr)
candidates <- c(0.1, 0.25, 0.5, 0.75, 0.9)

sim_list <- vector(mode = "list",
                  length = length(candidates))
for(c in seq_along(candidates)){
  
  sim_list[[c]] <- rbinom(100, 100, candidates[c])
  
}
names(sim_list) <- as.factor(candidates)
sim_df <- as_tibble(sim_list) %>%
  pivot_longer(names_to = "prob",
               values_to = "successes",
               cols = everything())

sim_df %>%
  ggplot(aes(x = successes, 
             fill = prob, 
             color = prob)) + 
  geom_histogram(alpha = 0.1,
                 position = 'identity') +
  geom_histogram(data = tibble(successes = simdata,
                               prob = factor('data')),
                 aes(x = successes),
                 color = 'black',
                 alpha = 0.1,
                 fill = 'darkgray') +
  scale_fill_viridis_d() +
  scale_color_viridis_d() + 
  theme_classic() +
  ggtitle('data in black/gray') + 
  xlim(c(0, 100))


sim_df %>%
  ggplot(aes(x = successes, 
             fill = prob, 
             color = prob)) + 
  geom_density(alpha = 0.1,
                 position = 'identity') +
  geom_density(data = tibble(successes = simdata,
                               prob = factor('data')),
                 aes(x = successes),
                 color = 'black',
                 alpha = 0.1,
                 fill = 'darkgray') +
  scale_fill_viridis_d() +
  scale_color_viridis_d() + 
  theme_classic() +
  ggtitle('data in black/gray') + 
  xlim(c(0, 100))

```

Of these small subset of candidate `prob` values, 0.5 is the clear winner. The overlap of the simulation with the data is striking, and strongly suggests that this is a good fit to our data. 

Of course, we know the true value is 0.5, and this knowledge guided our choice of candidates. So let's explore a different range of candidates, all of which are much closer to the known truth:


```{r}
### With a for loop
library(tidyr)
candidates <- c(0.48, 0.49, 0.5, 0.51, 0.52)

sim_list <- vector(mode = "list",
                  length = length(candidates))
for(c in seq_along(candidates)){
  
  sim_list[[c]] <- rbinom(100, 100, candidates[c])
  
}
names(sim_list) <- as.factor(candidates)
sim_df <- as_tibble(sim_list) %>%
  pivot_longer(names_to = "prob",
               values_to = "successes",
               cols = everything())


sim_df %>%
  ggplot(aes(x = successes, 
             fill = prob, 
             color = prob)) + 
  geom_histogram(alpha = 0.1,
                 position = 'identity') +
  geom_histogram(data = tibble(successes = simdata,
                               prob = factor('data')),
                 aes(x = successes),
                 color = 'black',
                 alpha = 0.1,
                 fill = 'darkgray') +
  scale_fill_viridis_d() +
  scale_color_viridis_d() + 
  theme_classic() +
  ggtitle('data in black/gray') + 
  xlim(c(0, 100))

sim_df %>%
  ggplot(aes(x = successes, 
             fill = prob, 
             color = prob)) + 
  geom_density(alpha = 0.1,
                 position = 'identity') +
  geom_density(data = tibble(successes = simdata,
                               prob = factor('data')),
                 aes(x = successes),
                 color = 'black',
                 alpha = 0.1,
                 fill = 'darkgray') +
  scale_fill_viridis_d() +
  scale_color_viridis_d() + 
  theme_classic() +
  ggtitle('data in black/gray') + 
  xlim(c(0, 100))

```

Now the comparisons are much messier. Sure, 0.5 is a great match, but none of these chosen values yield simulations too different from the truth. 

From this, it is clear that there are ranges of values that we can confidently rule out as good conclusions for what `prob` best describes our data. Values of 0.25 or less, and values of 0.75 or more are very bad fits. There is also good evidence that a good fit is somewhere around 0.5, but the exact value that would represent the best guess is uncertain.

**Step 4: Make fitting more rigorous**

The current strategy we have employed could be referred to as "simulations and vibes". We have simulated data with some values for the unknown parameter in question, and assessed by eye how close our simulated data matched our real data. Whlie this has offered some valuable insights, it's important to recognize the fundamental limitations of such an approach:

1. Each simulation with a given parameter value will yield different results. Random number generators are going to be random. One output of `prob = 0.5` might look exactly like the real think, but so might one value of `prob = 0.48`.
2. It's time intensive. You have to simulate data for a bunch of different values, and then painstakingly stare at cluttered plots trying to make sense of which simulation fit your data the best.
3. It's subjective. None of our simulations matched the data exactly, and even if one run of a particular simulation did, see point 1 for why that can't be considered conclusive evidence in favor of that parameter being the best. Lacking such a perfect match,
we are forced to rely on ill-defined, self-imposed criteria for what the best fit is.

How can we fix this problem? What we need is a quantitative metric, a number that we can assign to every possible value of `prob` that describes how good of a fit it is to our data. It should have the following properties:

1. It should be deterministic; a given value of `prob` should yield a single unique value for this metric. 
1. Higher values should represent better fits.
1. The more data we have, the better this metric should do at predicting the true parameter value.

Sit and think about this for a while (maybe a couple decades, which is what it took the field to converge on this solution), and you'll arrive at something called the **"total likelihood"**. In this 
part of the exercise, we'll develop an intuition for this concept:

#### What is the likelihood?

When discussing a particular distribution, we have playing with its associated random number generator function in R. For the binomial distribution, this is the `rbinom()` function. If you check the documentation for `rbinom()` though, with `?rbinom`, you'll see that you actually get documentation for 4 different functions, all with a similar naming convention. We'll find use for all of these at some point in this class, but for now, let's focus on one that we briefly visited last week, `dbinom()`.

Unlike `rbinom()`, `dbinom()` returns the same value for a given set of parameters every single time. It is not random:

```{r, results = 'hold'}
dbinom(50, 100, 0.5)
dbinom(50, 100, 0.5)
```

What does this value represent though. First, let's understand its input:

1. `x`: The number of "successes". So essentially what you could get out from a given run of `rbinom()`.
2. `size`: Same as in `rbinom()`, the number of trials.
3. `prob`: Same as in `rbinom()`, the probability that each trial is a success.

So it seems to quantify something about a particular possible outcome of `rbinom()`. It assigns some number to an outcome given the parameters you could have used to simulate such an outcome. What does this number represent though? To find out, let's plot it for a range of `x`'s:

```{r}
xs <- 0:100
ds <- dbinom(xs, 100, 0.5)

tibble(x = xs,
       d = ds) %>%
  ggplot(aes(x = x, y = d)) + 
  geom_point() + 
  theme_classic()
```

Values closer to 50 get higher numbers than values further from 50, and the plot
seems to be symmetric around 50. What's significant about 50? It's $$size * prob$$,
or the average value we would expect to get from `rbinom()` with these parameters!

This investigation should give you some sense that the output of `dbinom()` is 
in some way related to the probability of seeing `x` given a certain `size` and
`prob`. Is it exactly this probability? We can gut check by assessing some cases
we know for certain. Like, what is the probability of seeing 1 success in 1 trial
if `prob` = 0.5? 0.5, because that's the defintiion of prob! What does `dbinom()`
give us in this scenario:

```{r}
dbinom(1, 1, 0.5)
```

0.5; that's a good sign that we are on to something. Try out some different values
of `prob`:

```{r}
dbinom(1, 1, 0.3)
dbinom(1, 1, 0.7)
dbinom(1, 1, 0.2315)
```

Everything still checks out. How about the probability of 2 successes in 2 trials
given a certain `prob`? Each trial has probability of `prob` of being a success,
and each trial is independent. Therefore, the probability of 2 successes in 2 trials
is $$prob * prob$$. Does `dbinom()` give us the same output in that case?

```{r}
# Expecting 0.25
dbinom(2, 2, 0.5)

# Expecting 0.01
dbinom(2, 2, 0.1)
```

Sure thing! 

**Conclusion**: `dbinom(x, size, prob)`, tells us the probability of seeing
`x` successes in `size` trials given the probability of a success is `prob`. This
is known as the "likelihood of our data".


#### Calculating the total likelihood

In the above examples, we took a single value of x, and passed it `dbinom()`. This
told us the probability of seeing that one value given whatever parameter values
we set. In most cases though, we typically have mulitple replicates of data. How
can we calculate the likelihood of our entire dataset? 

I'll pose a solution, and then demonstrate why the solution makes sense. The 
solution is to run `dbinom()` on each data point, and multiply the outcomes. This
can be done like so:

```{r}
data <- c(48, 50, 49, 50)
likelihood <- prod(dbinom(data, size = 100, prob = 0.5))
```

Why does this make sense? Consider the examples we went through above considering
cases where `size` was 1 or 2. If we have two datasets with a single trial each,
that's like having one dataset with two trials. Thus, the likelihood for the full
dataset of two single trial data points must be the same as that of a single dataset
with one two trial data point. In other words:

```{r}
datapoint1 <- c(1)
datapoint2 <- c(0)
dataset <- datapoint1 + datapoint2

dataset_L <- dbinom(dataset, size = 2, prob = 0.5)

calc_total_L <- function(datapoints, size = 2, prob = 0.5){
  
  ### Let's do what I suggested above: multiply the values
  return(prod(dbinom(datapoints, size = size, prob = prob)))
  
}

total_L <- calc_total_L(c(datapoint1, datapoint2), size = 1, prob = 0.5)

# We need this to be TRUE if our calculation is to be believed
total_L == dataset_L
```

Multiplying the likelihoods of the individual data points gives us the same
thing as the likelihood for the dataset generated from combining the results of 
the two individual trials! This is anecdotal, but we can formalize why this is 
true, and when it will be true:

**Conclusion**: If each of a collection of N datapoints is "independent", then
the total likelihood of the dataset is equal to the product of the individual
datapoint likelihoods. Datapoints being independent means that the value of any
given datapoint does not influence the values of any other datapoint.

**COMPUTATIONAL ASIDE**

Above, we calculate the total likelihood. One problem that we will often run into
with this calculation is that the product of all of the individual data point
likelihoods will be really small for some potential parameter values. In this case,
our computer might just assign these a value of 0, even though the true likelihood
is non-zero. This is called an "overflow error", and can mess with some of the things
we do later in this course.

For this reason, we will instead calculate the "log-total-likelihood". This is just the 
logarithm of the total likelihood. Why does this matter? All of the `d<distribution>()`
functions have one additional parameter named `log`, that can be set to either 
`TRUE` or `FALSE`. Setting it to `TRUE` will lead to it outputting the logarithm
of what it would otherwise output. 

In addition, the log of a product is the same as the sum of logs:

```{r}
numbers <- rbinom(100, size = 100, prob = 0.5)
ll_one_way <- log(prod(dbinom(numbers, 100, 0.5)))
ll_other_way <- sum(dbinom(numbers, 100, 0.5, log = TRUE))

ll_one_way == ll_other_way
```

So it is almost always preferable to sum the log-likelihoods of individual
datapoints than it is to multiply their regular likelihoods.

#### How can we use the total log-likelihood to fit models?

The total log-likelihood of our data turns out to be a great metric by which 
determine model fit. The idea is to find the parameter value that gives you the 
largest likelihood. That's to say, we choose as our best fit the parameter 
value(s) that make our data as probabilistically reasonable as possible. This is 
known as the maximum likelihood estimates for our parameter(s). If we simulated 
data using this value, our simulated data would on average come closer to looking 
like our real data than it would for any other parameter values we could have 
simulated with. 

How does this play out with our data? We can visualize the so-called log-likelihood
function to see which value of `prob` maximizes it:

```{r}
prob_guesses <- seq(from = 0, to = 1, by = 0.001)
log_likelihoods <- sapply(prob_guesses, function(x) sum(dbinom(simdata, 
                                                           size = 100, 
                                                           prob = x, 
                                                           log = TRUE)))

Ldf <- tibble(ll = log_likelihoods,
              prob = prob_guesses)

Ldf %>%
  ggplot(aes(x = prob, y = ll)) +
  geom_line() + 
  theme_classic()
```

Keep in mind the y-axis is a log scale, so differences of 1 equate to an order
of magnitude difference on the "regular" scale. Let's zoom in around our putative
values to get a better look at which value of `prob` is most likely:


```{r}
zoomed_plot <- Ldf %>%
  filter(prob >= 0.45 & prob <= 0.55) %>%
  ggplot(aes(x = prob, y = ll)) +
  geom_line() + 
  theme_classic()
zoomed_plot
```

It looks like a value of `prob = 0.5` is not strictly speaking the best guess.
Let's see what value in our grid of `prob_guesses` gives the highest value, and
annotate this value on our zoomed in plot:

```{r}
max_ll <- Ldf %>% filter(ll == max(ll)) %>%
  dplyr::select(prob) %>% unlist() %>% unname()
max_ll
zoomed_plot +
  geom_vline(xintercept = max_ll,
             color = 'darkred')
```

So our best guess given this metric is a value for `prob` of around 0.497. That's
pretty darn close to the true value of 0.5!


#### How we ACTUALLY find the maximum likelihood

While the strategy for finding the maximum likelihood parameter estimate used 
above is ok for this toy example, we can do a lot better in terms of the speed and
generalizability of our computational solution. If we can provide the function that
we want to maximize (e.g., the log-likelihood), and the data needed to calculate
the value of our function for a given parameter estimate, then R provides us
with the tool necessary to find the parameter estimate that maximizes our function.
That tool is the `optim()` function.

Here is how you can use `optim()` to find the maximum likelihood estimate of 
`prob` in our example:

```{r}
### Step 1: Define the function to maximize
binom_ll <- function(params, data, size = 100){
  
  # We estimate on a log scale for convenience so we convert here
  prob <- params[1]
  
  # Log-likelihood assuming binomial distribution
  ll <- dbinom(data, 
               size = size, 
               prob = prob, 
               log = TRUE)
  
  # Return negative because optim() by default will find the minimum
  return(-sum(ll))
  
}


### Step 2: run optim()
fit <- optim(
  par = 0.5, # initial guess for prob
  fn = binom_ll, # function to maximize
  data = simdata, # something passed to the function
  method = "L-BFGS-B", # Most efficient method for finding max,
  lower = 0.001, # Lower bound for parameter estimate
  upper = 0.999 # Upper bound for parameter estimate
  )


### Step 3: Inspect the estimate
prob_est <- fit$par[1]
prob_est

```

